Vậy bây giờ chúng ta sẽ nhanh chóng xem qua về hyperparameters.

Các tham số hyper là cách để điều chỉnh các mạng nơ-ron của chúng ta sao cho có thể khai thác tối đa hiệu quả từ chúng.

Và tôi sẽ bao gồm một danh sách ở đây với một số hyperparameters phổ biến nhất.

Những tham số này sẽ rất quen thuộc với bạn.

Ngay khi bạn bắt đầu tạo ra các mạng nơ-ron của riêng mình, bạn sẽ rất nhanh chóng quen với việc điều chỉnh

các tham số hyper này.

Và thực sự rất dễ làm điều đó trong mã của bạn.

Nó thực sự rất đơn giản, vì vậy đừng quá lo lắng về điều đó.

Điều quan trọng lúc này là bạn ghi chú lại các loại tham số hyper khác nhau ở đâu đó.

Điều thực sự quan trọng là bạn hiểu chúng hoạt động như thế nào và tại sao việc điều chỉnh chúng lại quan trọng.

Vậy chúng ta sẽ đi qua từng cái một.

Số lượng lớp ẩn, tôi nghĩ điều này khá rõ ràng.

Nó giống như số lượng các lớp mà chúng ta có ở đây.

Vậy tại sao lại quan trọng khi thay đổi điều này?

Vâng, nếu chúng ta tăng số lượng lớp ẩn.

Hiện tại chúng ta có ba lớp, nhưng bây giờ chúng ta có thể có thêm năm, mười hay một trăm lớp.

Điều đó không quan trọng.

Dĩ nhiên là có quan trọng, nhưng bây giờ tất cả đều có thể.

Lý do tại sao chúng ta cần lớp ẩn.

Vì vậy, càng có nhiều lớp ẩn, chúng ta có thể lưu trữ được nhiều phức tạp hơn trong các phương trình của mình.

Về cơ bản, chúng ta có thể trích xuất các mẫu phức tạp hơn từ dữ liệu của mình.

Đó là một ưu điểm rất lớn và rất hữu ích.

Nhưng những gì làm cho nó trở nên ít hữu ích hơn, nhược điểm của việc có quá nhiều lớp ẩn là nó rất tốn kém về mặt tính toán

nên sẽ mất nhiều thời gian hơn để huấn luyện mạng của bạn.

Đó là một khó khăn.

Nhưng cũng một trong những điều quan trọng mà chúng ta đã đề cập trước đây và sẽ nói rất nhiều lần nữa là overfitting dữ liệu.

Đây là một vấn đề thực sự khi bạn huấn luyện mạng nơ-ron và bạn muốn tránh nó càng nhiều càng tốt, bạn muốn tránh overfitting.

Vì vậy, bạn cần phải tìm ra một sự cân bằng nào đó, đúng không?

Bởi vì bạn muốn trích xuất các mẫu phức tạp.

Đó là lý do tại sao chúng ta sử dụng mạng nơ-ron sâu.

Nhưng bạn không muốn nó quá phức tạp đến mức làm khớp quá hoàn hảo với dữ liệu huấn luyện của chúng ta.

Và sau đó không còn chỗ cho dữ liệu mới.

Vì vậy, lợi ích của lớp ẩn và việc tăng số lượng là nó tăng độ phức tạp.

Nhược điểm là nó tốn kém về mặt tính toán và dễ bị overfitting nếu có quá nhiều lớp ẩn.

Điều tương tự cũng áp dụng cho số lượng nút trong một lớp, càng có nhiều nút.

Vì vậy, chúng ta có thể thêm càng nhiều càng tốt.

Bạn thường có số lượng nhiều nhất ở giữa mạng nơ-ron của mình.

Và điều bạn không muốn là có chỉ hai nút trong một lớp ẩn của bạn.

Rồi sau đó bạn lại có rất nhiều và lại giảm xuống hai ngay lập tức.

Nói chung, bạn muốn thay đổi một cách dần dần hơn thay vì những thay đổi cực đoan

bởi vì nếu thay đổi quá cực đoan thì bạn sẽ mất rất nhiều thông tin.

Vì vậy, điều đó cần lưu ý.

Và vâng, thực ra thì nó giống như vậy.

Việc có quá nhiều nút trong một lớp sẽ rất tốn kém về tính toán, nhưng nó sẽ giúp tăng độ phức tạp cho kiến trúc mạng nơ-ron sâu của chúng ta, nhưng nó cũng dễ bị overfitting.

Tiếp theo là số lượng epochs.

Như chúng ta đã mô tả trước đó, epochs là khi chúng ta thực hiện một lần lan truyền tiến và sau đó một lần lan truyền ngược.

Vì vậy, thường thì bạn muốn số lượng này khá cao.

Chúng ta sẽ thử với rất nhiều số lượng khác nhau, nhưng thường thì tốt hơn khi có một số lượng epoch cao

bởi vì về cơ bản, bạn biết đấy, càng huấn luyện nhiều thì càng tốt.

Tuy nhiên, có thể vào một lúc nào đó trong quá trình huấn luyện, kết quả sẽ tệ hơn.

Và đó là điều cần lưu ý.

Vì vậy, chúng ta sẽ đi qua cách tạo một đồ thị để hiển thị tổn thất theo số lượng epochs.

Vì vậy, có thể bắt đầu rất cao và sau đó tổn thất giảm, điều đó rất tốt, nhưng sau đó lại bắt đầu tăng lên.

Vậy là chúng ta có thể nói, được rồi, epoch số 500 gì đó.

Ok, vào thời điểm này, chúng ta sẽ dừng lại.

Một điều quan trọng là nếu vào một thời điểm nào đó, việc có thêm nhiều epochs lại không hữu ích và có thể làm tăng tổn thất.

Điều khác là nó tốn thời gian để huấn luyện.

Vì vậy, rất thường xuyên, đặc biệt là khi bạn thử nghiệm với các kiến trúc khác nhau và khi tôi nói về các kiến trúc, tôi có nghĩa là số lượng lớp ẩn, tất cả các tham số hyper của bạn, tất cả những thứ đó.

Bạn có thể muốn thực hiện một số lượng epochs thấp để tiết kiệm thời gian.

Và khi bạn thực sự muốn đi với một kiến trúc cụ thể và bạn sẽ để nó huấn luyện qua đêm gì đó, thì bạn có thể tăng số lượng epochs.

Tỷ lệ học là rất quan trọng.

Và chúng ta đã chạm qua rất nhanh về nó.

Nhưng thực chất, khi chúng ta nói về tổn thất và nếu vẽ đồ thị ra.

Vì vậy, giả sử đây là tổn thất của chúng ta, nó giảm dần và sau đó tăng lên.

Đây là dựa trên các tham số mà chúng ta đang sử dụng.

Và bạn có thể thấy, được rồi, đây là đáy ở đây.

Đây là nơi chúng ta muốn tổn thất của mình đi đến.

Nhưng nếu chương trình của chúng ta, thuật toán của chúng ta bị kẹt ở đây thì sao?

Vậy chúng ta sẽ phóng to vào chỗ này.

Vậy nó sẽ đến đáy ở đây và nhiệm vụ là giảm thiểu tổn thất, đúng không?

Đó là những gì chúng ta đang cố gắng làm với lan truyền ngược.

Vậy khi đến điểm này, chương trình có thể nghĩ, bạn biết đấy, nếu tôi đi theo hướng này hoặc

nếu tôi đi theo hướng này, tổn thất sẽ tăng lên.

Vậy đây là điều tốt nhất mà chúng ta có thể làm để giảm thiểu tổn thất, trong khi thực tế, đây chỉ là một cực tiểu địa phương.

Vậy đó là một từ khóa quan trọng để ghi nhớ.

Vậy cực tiểu địa phương và đây là cực tiểu toàn cầu ở dưới đây, thực sự là cực tiểu thực sự.

Vậy điều quan trọng là tỷ lệ học.

Nếu bạn có tỷ lệ học thấp, thì về cơ bản nó chỉ đi theo hướng này, thay đổi một chút thôi.

Và đây chính là mức độ bạn cập nhật các tham số mỗi lần.

Vì vậy, nếu bạn có tỷ lệ học thấp, thì khi thực hiện lan truyền ngược, trọng số của bạn chỉ thay đổi một chút.

Và đó là điều hữu ích ở đây.

Bạn có thể từ từ giảm xuống, từ từ giảm xuống, nhưng khi nó đến đây, vì tỷ lệ học quá thấp, nó sẽ cố gắng đi theo một cách nhỏ và nó sẽ bị kẹt trong cực tiểu địa phương này.

Trong khi nếu bạn có tỷ lệ học lớn hơn, ví dụ như độ dài này, nó sẽ đến đây và nói, ok, chúng ta có tỷ lệ học này hoặc chúng ta có thể đi xa hơn, và đó là lúc nó bắt đầu nhìn xa hơn.

Vì vậy, điều tốt của việc có tỷ lệ học lớn là bạn có thể tránh được cực tiểu địa phương tốt hơn.

Nhưng điều cũng có vấn đề là nó có thể quá lớn đến mức không bao giờ có thể tìm ra cực tiểu toàn cầu.

Cực tiểu toàn cầu, xin lỗi, vì nó quá lớn và không thể vào chi tiết được.

Vì vậy, điều tốt khi có tỷ lệ học cao là trọng số và độ lệch của bạn được cập nhật

ở mức độ lớn hơn.

Tỷ lệ học thấp có nghĩa là mỗi epoch,

trọng số và độ lệch chỉ được cập nhật một chút.

Ưu điểm của tỷ lệ học lớn là bạn tránh được cực tiểu địa phương, nhưng nó thường bỏ qua cực tiểu toàn cầu vì tỷ lệ học quá lớn đến mức không thể đi đến đó.

Một điều tốt để có là một số người đã bắt đầu sử dụng một cái gọi là học giảm dần,

về cơ bản bạn bắt đầu với một tỷ lệ học lớn và sau đó qua các epoch, tỷ lệ học thực sự giảm xuống, đó là một ý tưởng hay và là một điều cần lưu ý.

Tiếp theo là kích thước batch.

Nó chỉ đơn giản là số lượng dữ liệu.

Vậy giả sử chúng ta đang làm dự đoán giá nhà, bao nhiêu, bao nhiêu ngôi nhà thông tin về chúng

được đưa vào một lần.

Vì vậy, nếu bạn cố gắng đưa vào một nghìn, có thể nó sẽ quá nhiều đối với máy tính và có thể gây lỗi thiếu bộ nhớ hoặc gì đó.

Vì vậy, đây là một tham số hyper tương đối nhỏ cần lưu ý, nhưng nó vẫn hữu ích để giữ kích thước batch tương đối nhỏ, nhưng đủ lớn để duy trì hiệu quả.

Tiếp theo, chúng ta có hàm kích hoạt.

Cho đến nay chúng ta chỉ nói về việc sử dụng sigmoid.

Cái này ở đây, mà bạn biết đấy, tạo ra phi tuyến tính.

Sau khi thực hiện hồi quy tuyến tính, có rất nhiều hàm kích hoạt khác mà chúng ta sẽ đi vào khi thực hiện các dự án thực tế.

Vì vậy, đừng quá lo lắng về điều đó.

Tôi sẽ giới thiệu các tên cho bạn.

Và khi đến phần mã, chỉ đơn giản là thay đổi một dòng mã.

Nhưng thực tế là việc sử dụng các hàm kích hoạt khác nhau cho các mục đích khác nhau là rất quan trọng.

Sigmoid rất hữu ích nếu bạn muốn kết quả đầu ra là 0 hoặc 1.

Vậy ví dụ, trong trường hợp Titanic, người nào sống sót 0 hay 1?

Đó là lúc sigmoid thực sự hữu ích, còn đối với các hàm khác,

chúng ta có các thứ như Relu.

Relu rỉ, và tanh.

Có những hàm khác bạn không cần phải lo lắng quá nhiều lúc này, nhưng chúng ta sẽ đi vào nó trong tương lai.

Điều tiếp theo là kỹ thuật backpropagation.

Một cái tôi đã đề cập là gradient descent, và đó là một tên gọi khá trực quan đúng không?

Bởi vì bạn đang tìm kiếm nơi mà độ dốc của tổn thất giảm xuống, nhưng cũng có những cái khác.

Tôi sẽ giới thiệu chúng cho bạn trong tương lai.

Cuối cùng, chúng ta có kỹ thuật regularization.

Ví dụ, dropout là một ý tưởng khá thú vị.

Về cơ bản, nếu bạn phát hiện ra dữ liệu của mình bị overfitting, mô hình của bạn bị overfitting, thì dropout thực hiện một điều khá cơ bản nhưng thú vị và thực sự rất hiệu quả.

Dropout về cơ bản là mỗi lần bạn thực hiện một epoch, nó sẽ quyết định ngẫu nhiên để "giết" đi một số nút của bạn.

Nó thực sự chỉ việc "giết" chúng đi, loại bỏ trọng số và độ lệch của chúng một cách ngẫu nhiên.

Vì vậy, mỗi lần có những nút ngẫu nhiên bị loại bỏ và bạn có thể thay đổi tỷ lệ dropout của mình.

Vậy 50% có nghĩa là thực tế 50% các nút của bạn mỗi lần bị tắt.

Điều đó khá nhiều đối với nhiều người.

Tuy nhiên, nó thực sự khá hiệu quả và có những kỹ thuật regularization khác mà chúng ta sẽ nói đến trong tương lai, nhưng đó là các tham số hyper chính.

Hãy ghi chú lại mỗi cái và cố gắng ghi nhớ một số ưu và nhược điểm của việc có các lớp ẩn cao, tỷ lệ học cao, tỷ lệ học thấp, v.v.

Để trở nên quen thuộc với chúng.

Và khi bạn thực hiện càng nhiều dự án và cần điều chỉnh chúng ngày càng nhiều, bạn sẽ cảm thấy nó trở nên rất trực quan.

Vậy hãy thử nhắc lại những điều này và những ưu nhược điểm, rồi tôi sẽ gặp bạn trong bài học tiếp theo.