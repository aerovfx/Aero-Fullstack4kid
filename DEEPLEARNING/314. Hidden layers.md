Để tính giá trị Y ở đây, thực chất chỉ cần:
Y=Y 1 +Y 2 +Y 3
​ 
Đơn giản như vậy. Vậy nên, dù bạn có bao nhiêu node kết nối tới node mà chúng ta đang tính toán, tất cả chỉ là các hồi quy tuyến tính và sau đó cộng lại, vậy nên chúng ta có thể viết:

Giá trị Y=∑c 
a
ˊ
 c gi 
a
ˊ
  trị Y

Điều quan trọng cần lưu ý ở đây là chúng ta đang cộng các hồi quy tuyến tính lại với nhau. Và đây chính là phần rất quan trọng: mỗi hồi quy tuyến tính trong mạng neural này, tất cả các đường nối khác nhau, sẽ có trọng số khác nhau. Tất nhiên, chúng có thể giống nhau ở một thời điểm nào đó vì ngẫu nhiên, ví dụ như trọng số của một số kết nối có thể là 0.8, và kết nối khác cũng có thể là 0.8. Nhưng chúng hoàn toàn độc lập với nhau.

Mỗi kết nối sẽ có trọng số riêng và sẽ được cập nhật riêng biệt, nhưng trong cùng một lớp ẩn, chúng ta sẽ có cùng một bias. Nếu nhìn lại phương trình:


Y=W⋅X+B

Bias sẽ giữ nguyên trong một lớp ẩn, nhưng trọng số 

W sẽ độc lập với nhau, và điều này giúp tạo ra sự phức tạp cho mạng neural. Mục đích chính của mạng neural là cho phép mô hình phát hiện các mẫu ẩn phức tạp hơn.

Tất nhiên, lúc đầu mọi thứ đều khá ngẫu nhiên. Điều duy nhất thực sự điều chỉnh cách thay đổi các trọng số và bias giữa các lớp ẩn chính là lỗi (loss) ở cuối quá trình. Khi chúng ta đến lớp output, tính toán độ sai lệch (loss), sự khác biệt giữa dự đoán của chúng ta (Y hat) và giá trị thực tế (Y). Máy tính sẽ so sánh chúng và cập nhật trọng số để giảm thiểu lỗi. Đây giống như khi chúng ta vẽ đường hồi quy tối ưu, bắt đầu từ một điểm và điều chỉnh để giảm độ sai lệch.

Việc này rất khó trực quan hóa trên đồ thị vì có quá nhiều trục toạ độ, vì vậy không thể làm được dễ dàng.

Điều tôi muốn nhấn mạnh trong bài giảng này là bạn có thể thêm bao nhiêu lớp ẩn và bao nhiêu node trong mỗi lớp ẩn tùy ý. Tuy nhiên, việc này sẽ rất tốn kém về mặt tính toán, vì mỗi kết nối đều phải thực hiện hồi quy tuyến tính với trọng số độc lập và cùng một bias trong một lớp ẩn. Để tính toán giá trị của mỗi node, máy tính sẽ thực hiện phép hồi quy tuyến tính cho mỗi kết nối và cộng lại.

Một số bạn có thể nghĩ rằng, “Mình chỉ muốn lập trình mạng neural thôi, để máy tính thực hiện các phép tính cho mình mà không cần lo lắng.” Tuy nhiên, hiểu lý thuyết cơ bản đằng sau mạng neural sẽ giúp bạn hiểu tại sao nó lại tốn kém về tính toán, và cái gì đang thực sự xảy ra bên dưới. Điều quan trọng là việc này tạo ra dự đoán của chúng ta và giúp chúng ta tính toán độ sai lệch (loss).

Vì vậy, việc hiểu lý thuyết này sẽ giúp bạn xây dựng các mạng neural tốt hơn.

Những điểm quan trọng cần lưu ý trong bài giảng này là: Trong mạng neural sâu của bạn, lớp đầu vào (input layer) nằm ở bên trái, ví dụ như trong bài toán giá nhà, sẽ là các biến độc lập như số phòng, tuổi của ngôi nhà. Lớp output sẽ chỉ có một node duy nhất, là giá trị dự đoán giá nhà. Các lớp ở giữa là các lớp ẩn, trong đó có nhiều node kết nối với nhau, mỗi node thực hiện hồi quy tuyến tính với trọng số độc lập, nhưng cùng một bias trong mỗi lớp ẩn.

Hãy ghi nhớ những điểm quan trọng này và tôi sẽ gặp bạn trong bài giảng tiếp theo.