T√¥i r·∫•t h√°o h·ª©c gi·ªõi thi·ªáu ƒë·∫øn b·∫°n m·∫°ng n∆°-ron ƒë·∫ßu ti√™n c·ªßa ch√∫ng ta. B·∫°n c√≥ th·ªÉ nghƒ© r·∫±ng, "Wow, ƒë√¢y ch·∫Øc h·∫≥n l√† m·ªôt b∆∞·ªõc nh·∫£y l·ªõn t·ª´ nh·ªØng g√¨ t√¥i ƒë√£ h·ªçc." Nh∆∞ng t√¥i ƒë·∫£m b·∫£o v·ªõi b·∫°n r·∫±ng kh√¥ng ph·∫£i nh∆∞ v·∫≠y. ƒê√¢y ch·ªâ l√† m·ªôt b∆∞·ªõc nh·ªè, r·∫•t nh·ªè th√¥i.

Ch√∫ng ta s·∫Ω b·∫Øt ƒë·∫ßu t√¨m hi·ªÉu v·ªÅ m·∫°ng n∆°-ron s√¢u. H√£y nh√¨n v√†o s∆° ƒë·ªì n√†y ‚Äì c√≥ l·∫Ω b·∫°n ƒë√£ t·ª´ng th·∫•y th·ª© g√¨ ƒë√≥ t∆∞∆°ng t·ª± tr∆∞·ªõc ƒë√¢y: tr√¥ng r·∫•t ƒë√°ng s·ª£ v·ªõi nhi·ªÅu ƒë∆∞·ªùng v√† nh·ªØng th·ª© ƒë∆∞·ª£c g·ªçi l√† "l·ªõp ·∫©n". Th∆∞·ªùng th√¨, khi t√¥i h·ªèi ai ƒë√≥ v·∫Ω ra m·∫°ng n∆°-ron m√† h·ªç h√¨nh dung, h·ªç s·∫Ω v·∫Ω r·∫•t nhi·ªÅu ƒë∆∞·ªùng ch·∫±ng ch·ªãt nh∆∞ v·∫≠y. Nh∆∞ng th·ª±c ra, n√≥ kh√¥ng ph·ª©c t·∫°p nh∆∞ b·∫°n nghƒ©, v√† t√¥i s·∫Ω h∆∞·ªõng d·∫´n b·∫°n t·ª´ng b∆∞·ªõc ngay b√¢y gi·ªù.

ƒê√¢y l√† m·ªôt s∆° ƒë·ªì t√¥i ƒë√£ v·∫Ω tr∆∞·ªõc ƒë√≥, ch·∫Øc ch·∫Øn tr√¥ng ƒë·∫πp h∆°n s∆° ƒë·ªì ban ƒë·∫ßu kia. Ch√∫ng ta s·∫Ω ph√¢n t√≠ch n√≥: c√≥ m·ªôt l·ªõp ƒë·∫ßu v√†o, c√°c l·ªõp ·∫©n, v√† m·ªôt l·ªõp ƒë·∫ßu ra. B√¢y gi·ªù, ch√∫ng ta ch·ªâ c·∫ßn n·∫Øm b·∫Øt c√°c kh√°i ni·ªám c∆° b·∫£n.

L·ªõp ƒë·∫ßu v√†o:
Gi·∫£ s·ª≠ ch√∫ng ta ƒëang t·∫°o m·ªôt m√¥ h√¨nh d·ª± ƒëo√°n gi√° nh√† v√† ch·ªâ s·ª≠ d·ª•ng m·ªôt th√¥ng s·ªë duy nh·∫•t l√† s·ªë ph√≤ng. V√≠ d·ª•, ƒë·∫ßu v√†o c·ªßa ch√∫ng ta l√† s·ªë ph√≤ng = 4. Ch√∫ng ta c≈©ng c√≥ th·ªÉ th√™m c√°c y·∫øu t·ªë kh√°c nh∆∞ tu·ªïi c·ªßa ng√¥i nh√†, nh∆∞ng hi·ªán t·∫°i h√£y gi·ªØ m·ªçi th·ª© ƒë∆°n gi·∫£n.

L·ªõp ·∫©n:
M·ªói h√¨nh tr√≤n trong l·ªõp ·∫©n l√† m·ªôt n√∫t (node). ƒê√¢y l√† n∆°i c√°c ph√©p t√≠nh ƒë∆∞·ª£c th·ª±c hi·ªán, v√† ch√∫ng r·∫•t ƒë∆°n gi·∫£n, t∆∞∆°ng t·ª± nh∆∞ h·ªìi quy tuy·∫øn t√≠nh. M·ªói k·∫øt n·ªëi l√† m·ªôt ph√©p t√≠nh 
y=w‚ãÖx+b, trong ƒë√≥:

w: tr·ªçng s·ªë (weight), kh·ªüi t·∫°o ng·∫´u nhi√™n.
b: h·∫±ng s·ªë b√π (bias), c≈©ng ƒë∆∞·ª£c kh·ªüi t·∫°o ng·∫´u nhi√™n.
K·∫øt qu·∫£ c·ªßa ph√©p t√≠nh 
y s·∫Ω l√† ƒë·∫ßu v√†o cho n√∫t ti·∫øp theo. Nh∆∞ng n·∫øu ch·ªâ s·ª≠ d·ª•ng h·ªìi quy tuy·∫øn t√≠nh, t·∫•t c·∫£ ph√©p t√≠nh s·∫Ω ch·ªâ t·∫°o ra m·ªôt ƒë∆∞·ªùng th·∫≥ng. ƒê·ªÉ m·∫°ng n∆°-ron c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c c√°c m·ªëi quan h·ªá phi tuy·∫øn t√≠nh, ch√∫ng ta c·∫ßn th√™m m·ªôt th√†nh ph·∫ßn phi tuy·∫øn g·ªçi l√† h√†m k√≠ch ho·∫°t (activation function).

M·ªôt trong nh·ªØng h√†m k√≠ch ho·∫°t ph·ªï bi·∫øn nh·∫•t l√† sigmoid, c√≥ d·∫°ng ƒë·ªì th·ªã:

Sigmoid chuy·ªÉn gi√° tr·ªã y th√†nh gi√° tr·ªã z trong kho·∫£ng t·ª´ 0 ƒë·∫øn 1, gi√∫p m·∫°ng n∆°-ron m√¥ h√¨nh h√≥a c√°c m·ªëi quan h·ªá ph·ª©c t·∫°p h∆°n.
L·ªõp ƒë·∫ßu ra:
Sau khi ƒëi qua c√°c l·ªõp ·∫©n, l·ªõp ƒë·∫ßu ra s·∫Ω ƒë∆∞a ra d·ª± ƒëo√°n cu·ªëi c√πng, v√≠ d·ª•, gi√° nh√†. K·∫øt qu·∫£ n√†y ƒë∆∞·ª£c t·∫°o ra b·∫±ng c√°ch ti·∫øp t·ª•c √°p d·ª•ng c√°c ph√©p t√≠nh 
y=w‚ãÖx+b v√† h√†m k√≠ch ho·∫°t.
K·∫øt lu·∫≠n:
Nh∆∞ v·∫≠y, m·∫°ng n∆°-ron th·ª±c ch·∫•t ch·ªâ l√† s·ª± k·∫øt h·ª£p c·ªßa nhi·ªÅu ph√©p h·ªìi quy tuy·∫øn t√≠nh v√† c√°c h√†m k√≠ch ho·∫°t. C√°c tr·ªçng s·ªë w v√† h·∫±ng s·ªë b trong m·ªói k·∫øt n·ªëi ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a th√¥ng qua qu√° tr√¨nh hu·∫•n luy·ªán, gi√∫p m·∫°ng h·ªçc ƒë∆∞·ª£c c√°c m·∫´u t·ª´ d·ªØ li·ªáu.

Ch√∫c m·ª´ng b·∫°n ƒë√£ b∆∞·ªõc nh·ªØng b∆∞·ªõc ƒë·∫ßu ti√™n trong vi·ªác hi·ªÉu v·ªÅ m·∫°ng n∆°-ron! B·∫°n ƒë√£ s·∫µn s√†ng ƒë·ªÉ t√¨m hi·ªÉu s√¢u h∆°n trong b√†i gi·∫£ng ti·∫øp theo. üéâ


I'm very excited to introduce you to our first neural network and you might think, okay, wow, this

is going to be taking a really big jump from what I've already learned.

I can absolutely assure you it's not.

So it's just going to be a baby step.

Very small one.

And we'll actually be now talking about deep neural networks.

So here's a diagram of probably what you've seen before, something looking very intimidating with lots

of crazy lines and things called hidden layers.

You know, this is what when I ask someone to draw what they think neural networks are, when they don't

know much about it, they tend to draw something with lots and lots of lines.

Actually, it's really not that complicated and I'm going to lead you through it right now.

So here's one I drew earlier, which is obviously looks so much more better, so much better than this

one.

So essentially, we're going to break this down.

So we have what we call an input layer.

We have hidden layers and we have an output layer.

Now, I'm just going to go over the basics at this point, and there's not that much more that we need

to talk about, but we'll just talk about the basics for now.

The input layer is, for example, let's say we're creating a house price predictor and we're just going

to keep it really simple for now.

So we're just going have one thing here saying number of rooms.

So we want to predict the price of a house.

And the input we're giving is the number of rooms.

Now, of course we could.

Where's my little draw?

So we could create a number of other inputs.

This could be, I don't know, let's say age of House and stuff like that.

Of course, we can have lots of different inputs and we will get on to that, so don't worry about it.

But for now we're going to keep it a bit more simple and it will be a very easy transition to add more,

more parameters.

So next up are our hidden layers.

So these things here, each of these circles we can refer to as a node.

We'll be going.

We'll be going sort of zooming into one of these in a second.

But essentially, this is where all the calculations go on.

And the calculations are very simple, similar to our linear regressions.

And then at the end is our output layer.

And this will be our prediction.

So, for example, what we might have here is the price.

And that's.

So essentially we have our input here.

Then we do a lot of calculations in our hidden layers, and then we have an output layer, which is

our prediction, and that is a neural network.

That's all there is to it.

There's nothing great, nothing crazy about it that I haven't told you about.

But now we need to look into what these calculations actually do.

So we're all we're going to be looking at now is our input layer and then one of these nodes and then

another one after it.

So we start off with our input layer and then essentially what we're doing at the start here is, yep,

you've guessed it, linear regression.

So I know it's kind of difficult to believe, but actually that is kind of what happens here.

So we've got y equals w x.

Plus B.

And this weight is going to be randomly initialized.

So it's just going to be a completely random weight to begin with.

So that's the first thing that happened.

So we have our input layer.

And of course, like I've said, like I've said before, we can have a number of different inputs and

we're going to go over that later on.

But for now we're just going to ignore that.

So we have our number.

Let's say we have the number of rooms, which is four.

So we basically going to going to do Y equals and then a random wait times four plus B, which is our

bias.

And that will also be random unless we specify it.

So we're going to have Y in here, we're going to have a value for Y, and now we actually want to do

something else here.

We want to make that into a new number.

But we're going to call this number Z.

And essentially what because what we need to do, if you think about it, this is going to be w, y

equals W, X plus B, and then we're going to do the same calculation here.

We can we can separate these by calling this Y one.

We can call this Y2.

Equals w.

And then X will actually be our Y.

If you think about it, because that's what we're putting in.

Our input is going to be this Y.

So we could do WY1.

Plus B.

Now, that step might have been slightly confusing just with the Y.

Hopefully it wasn't.

But just to make sure you understand, X is always our input.

Our input here is four and then we work out what y is here.

Now to put this into our next node.

Obviously our input is going to be y, so that's why one, that's y.

X is now y one.

But if we were just to do linear regression the whole way, if you really stop and think about it,

if you do this calculation y equals W, x plus B, then y two equals W, y plus B, etcetera.

What you realize is it's still it's all linear and so it actually won't make any changes and it will

just be one straight line, which is essentially the same as just having one equation called y equals

w, x plus B And so that's that would just be doing linear linear regression but with more steps.

So we need to introduce something called a non-linearity.

I'm definitely going to type that out.

Otherwise you'll be spending five minutes watching me trying to write out non-linearity.

Non-linearity and what for that?

The basic one we use is something called sigmoid.

So I'm just going to draw that out for you in a graph.

So sigmoid has a shape like this.

Something like this anyway.

No, that's awful.

Let me try again.

Kind of like this.

It can definitely be drawn better.

I definitely suggest you do a Google image search.

But what this does, if we have zero here, we have one here.

So it introduces a non-linearity, which basically means that now that we have Y, we want to convert

that to a number between 0 and 1 based on what value y is.

It will be a number somewhere on this non-linear graph.

So this Y will be converted to Z and Z will be between 0 and 1.

So what have we done in this first step?

We've done linear regression y equals W, x plus B, and then we've converted y to a number between

0 and 1 using our sigmoid graph.

And that gives us Z and we're just calling it Z just because it's a different number.

And we want to we want to signify that it's a different number.

So instead of it being Y two equals Y, one plus B, it's going to be Y two.

Equals w.

Z.

And then we can, because this is our first Ed, we can just put that as Z one plus B.

So that's it.

And so you may be wondering, right?

Is this w going to be the same as this W?

Well, that's that's where that's a really, really important point.

All of our W's are going to be different.

So when you look back at this graph here, we're essentially doing linear regression and adding our

sigmoid, which is our activation function.

So that's a very, very important word to remember.

Activation function.

So we're introducing our activation function sigmoid.

So sigmoid is just one type of activation function.

There are a number of different ones.

You really don't need to be concerned with them for now.

Just just remember that essentially what we're doing is between any of these nodes, we're essentially

just doing linear regression and then we're adding an activation function that converts our the output

of our linear regression into a number between usually between 0 and 1.

But what's important is that it's not linear, it's non-linear.

So that that means that we're not just going to have a straight line and it's actually going to be quite,

quite bendy, to use the technical term, of course.

So right here what we've done is we've worked out that one, any one of these lines, what we're doing

is we're doing linear regression and then we're adding an activation function to turn the number between

0 and 1.

With that, with that number, that's between 0 and 1.

We're then inputting it into the next one using linear regression.

And then again we're adding an activation function.

And every time we're doing linear regression, we're using a different W and we'll talk a bit more about

B essentially in the different layers we're using the same B within a layer, but they're different

between layers.

But that's not too important for now.

What's really, really important is that you're excited because we've, we've literally just cracked

the code of what a neural network is.

And that's essentially wherever you see a line, all that's happening there is that we're doing linear

regression and then we're adding an activation function that turns the number into a number between

0 and 1.

And we keep doing that even up to the point where we get to our output layer.

And so that's it for our our transformation from linear regression to deep neural networks.

And we'll be going into some some of the finer points.

But if you can, if you can get your head around that, then you're really, really well on your way.

So congratulations for basically stepping up to your first neural network, and I'll see you in the

next lecture.