{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11z58bl3meSzo6kFqkahMa35G5jmh2Wgt","timestamp":1675913141439},{"file_id":"1GFSsqP2BWz4gtq0e-nki00ZHSirXwFyY","timestamp":1592100981251},{"file_id":"1fgRNZxFag-YyLOhVke77Non19YiZ6raM","timestamp":1586659114015},{"file_id":"1Z2_uKAAvOtQVrUulcxmP6m9TD-Pegl0r","timestamp":1586656948105},{"file_id":"1MEfpnXIxxbw2tjSRc1hqkxb2HrTToTBn","timestamp":1586052797157}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"eclLG4xlJRIE"},"source":["# PIFuHD Demo: https://shunsukesaito.github.io/PIFuHD/\n","\n","![](https://shunsukesaito.github.io/PIFuHD/resources/images/pifuhd.gif)\n","\n","Made by [![Follow](https://img.shields.io/twitter/follow/psyth91?style=social)](https://twitter.com/psyth91)\n","\n","To see how the model works, visit the project repository.\n","\n","[![GitHub stars](https://img.shields.io/github/stars/facebookresearch/pifuhd?style=social)](https://github.com/facebookresearch/pifuhd)"]},{"cell_type":"markdown","metadata":{"id":"wmFdsTvLKtBO"},"source":["## Note\n","Make sure that your runtime type is 'Python 3 with GPU acceleration'. To do so, go to Edit > Notebook settings > Hardware Accelerator > Select \"GPU\"."]},{"cell_type":"markdown","metadata":{"id":"1TfPAtL4CyZw"},"source":["## More Info\n","- Paper: https://arxiv.org/pdf/2004.00452.pdf\n","- Repo: https://github.com/facebookresearch/pifuhd\n","- Project Page: https://shunsukesaito.github.io/PIFuHD/\n","- 1-minute/5-minute Presentation (see below)"]},{"cell_type":"code","metadata":{"id":"5DDpqpf2BABR","colab":{"base_uri":"https://localhost:8080/","height":931},"executionInfo":{"status":"ok","timestamp":1675913186409,"user_tz":-420,"elapsed":407,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"57b87d96-5a2e-43dc-9b90-780bfe6ce798"},"source":["import IPython\n","IPython.display.HTML('<h2>1-Minute Presentation</h2><iframe width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/-1XYTmm8HhE\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe><br><h2>5-Minute Presentation</h2><iframe width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/uEDqCxvF5yc\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<h2>1-Minute Presentation</h2><iframe width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/-1XYTmm8HhE\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe><br><h2>5-Minute Presentation</h2><iframe width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/uEDqCxvF5yc\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"8vZaAyhUJ9QC"},"source":["## Requirements\n","- Python 3\n","- PyTorch tested on 1.4.0\n","- json\n","- PIL\n","- skimage\n","- tqdm\n","- numpy\n","- cv2"]},{"cell_type":"markdown","metadata":{"id":"sfPDep8LlP_I"},"source":["## Help! I'm new to Google Colab\n","\n","You can check out the following youtube video on how to upload your own picture and run PIFuHD. **Note that with new update, you can upload your own picture more easily with GUI down below.**\n"]},{"cell_type":"code","metadata":{"id":"zaMP1EitljaA","colab":{"base_uri":"https://localhost:8080/","height":461},"executionInfo":{"status":"ok","timestamp":1675913212782,"user_tz":-420,"elapsed":433,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"cd9215b0-738a-4c06-ce44-01ac51f39e23"},"source":["import IPython\n","IPython.display.HTML('<iframe width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/LWDGR5v3-3o\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/IPython/core/display.py:701: UserWarning: Consider using IPython.display.IFrame instead\n","  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"]},{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<iframe width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/LWDGR5v3-3o\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"WYhlsDkg1Hwb"},"source":["## Clone PIFuHD repository"]},{"cell_type":"code","metadata":{"id":"BmpEwdOd1G1z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675913223937,"user_tz":-420,"elapsed":991,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"e4cb9d36-2849-442b-b9cb-a7b7cc751202"},"source":["!git clone https://github.com/facebookresearch/pifuhd"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'pifuhd'...\n","remote: Enumerating objects: 216, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 216 (delta 0), reused 0 (delta 0), pack-reused 213\u001b[K\n","Receiving objects: 100% (216/216), 406.47 KiB | 21.39 MiB/s, done.\n","Resolving deltas: 100% (104/104), done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"QvQm-A8ESKb2"},"source":["## Configure input data"]},{"cell_type":"code","metadata":{"id":"xvle9T10fB6g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675913229790,"user_tz":-420,"elapsed":379,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"85fe76ab-cbfc-4ec5-d422-fd63412bf91e"},"source":["cd /content/pifuhd/sample_images"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/pifuhd/sample_images\n"]}]},{"cell_type":"markdown","source":["# Mục mới"],"metadata":{"id":"-bTFJeb4NqGi"}},{"cell_type":"markdown","metadata":{"id":"9SI7Ye1JfIim"},"source":["**If you want to upload your own picture, run the next cell**. Otherwise, go to the next next cell. Currently PNG, JPEG files are supported."]},{"cell_type":"code","metadata":{"id":"jaV_7Yi8fM-B","colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"status":"ok","timestamp":1675913327634,"user_tz":-420,"elapsed":17152,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"35a25abc-937f-4de3-9576-97d8d5773a3f"},"source":["from google.colab import files\n","\n","filename = list(files.upload().keys())[0]"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-47146aff-2cc5-4d6d-a8cb-7ecc7ef84ca8\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-47146aff-2cc5-4d6d-a8cb-7ecc7ef84ca8\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving ssssssssss.PNG to ssssssssss (1).PNG\n","Saving 1621595394_ui_loot_operator_mp_western_t9cia_14_2_ok.png to 1621595394_ui_loot_operator_mp_western_t9cia_14_2_ok.png\n"]}]},{"cell_type":"code","metadata":{"id":"AEzmmB01SOZp","executionInfo":{"status":"ok","timestamp":1675913338344,"user_tz":-420,"elapsed":389,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}}},"source":["import os\n","\n","try:\n","  image_path = '/content/pifuhd/sample_images/%s' % filename\n","except:\n","  image_path = '/content/pifuhd/sample_images/test.png' # example image\n","image_dir = os.path.dirname(image_path)\n","file_name = os.path.splitext(os.path.basename(image_path))[0]\n","\n","# output pathes\n","obj_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.obj' % file_name\n","out_img_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.png' % file_name\n","video_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.mp4' % file_name\n","video_display_path = '/content/pifuhd/results/pifuhd_final/result_%s_256_display.mp4' % file_name"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"896EC7iQfXkj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675913346915,"user_tz":-420,"elapsed":403,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"45208046-0330-4c2f-ec20-2de16285df2d"},"source":["cd /content"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"markdown","metadata":{"id":"JbVmda9J5TDL"},"source":["## Preprocess (for cropping image)"]},{"cell_type":"code","metadata":{"id":"UtMjWGNU5STe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675913352059,"user_tz":-420,"elapsed":398,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"54452cd1-2d3c-4186-8551-f8086de8b773"},"source":["!git clone https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'lightweight-human-pose-estimation.pytorch'...\n","remote: Enumerating objects: 120, done.\u001b[K\n","Receiving objects:   0% (1/120)\rReceiving objects:   1% (2/120)\rReceiving objects:   2% (3/120)\rReceiving objects:   3% (4/120)\rReceiving objects:   4% (5/120)\rReceiving objects:   5% (6/120)\rReceiving objects:   6% (8/120)\rReceiving objects:   7% (9/120)\rReceiving objects:   8% (10/120)\rReceiving objects:   9% (11/120)\rReceiving objects:  10% (12/120)\rReceiving objects:  11% (14/120)\rReceiving objects:  12% (15/120)\rReceiving objects:  13% (16/120)\rReceiving objects:  14% (17/120)\rReceiving objects:  15% (18/120)\rReceiving objects:  16% (20/120)\rReceiving objects:  17% (21/120)\rReceiving objects:  18% (22/120)\rReceiving objects:  19% (23/120)\rReceiving objects:  20% (24/120)\rReceiving objects:  21% (26/120)\rReceiving objects:  22% (27/120)\rReceiving objects:  23% (28/120)\rReceiving objects:  24% (29/120)\rReceiving objects:  25% (30/120)\rReceiving objects:  26% (32/120)\rReceiving objects:  27% (33/120)\rReceiving objects:  28% (34/120)\rReceiving objects:  29% (35/120)\rReceiving objects:  30% (36/120)\rReceiving objects:  31% (38/120)\rReceiving objects:  32% (39/120)\rReceiving objects:  33% (40/120)\rReceiving objects:  34% (41/120)\rReceiving objects:  35% (42/120)\rReceiving objects:  36% (44/120)\rReceiving objects:  37% (45/120)\rReceiving objects:  38% (46/120)\rReceiving objects:  39% (47/120)\rReceiving objects:  40% (48/120)\rReceiving objects:  41% (50/120)\rReceiving objects:  42% (51/120)\rReceiving objects:  43% (52/120)\rReceiving objects:  44% (53/120)\rReceiving objects:  45% (54/120)\rReceiving objects:  46% (56/120)\rReceiving objects:  47% (57/120)\rReceiving objects:  48% (58/120)\rReceiving objects:  49% (59/120)\rReceiving objects:  50% (60/120)\rReceiving objects:  51% (62/120)\rReceiving objects:  52% (63/120)\rReceiving objects:  53% (64/120)\rReceiving objects:  54% (65/120)\rReceiving objects:  55% (66/120)\rremote: Total 120 (delta 0), reused 0 (delta 0), pack-reused 120\u001b[K\n","Receiving objects:  56% (68/120)\rReceiving objects:  57% (69/120)\rReceiving objects:  58% (70/120)\rReceiving objects:  59% (71/120)\rReceiving objects:  60% (72/120)\rReceiving objects:  61% (74/120)\rReceiving objects:  62% (75/120)\rReceiving objects:  63% (76/120)\rReceiving objects:  64% (77/120)\rReceiving objects:  65% (78/120)\rReceiving objects:  66% (80/120)\rReceiving objects:  67% (81/120)\rReceiving objects:  68% (82/120)\rReceiving objects:  69% (83/120)\rReceiving objects:  70% (84/120)\rReceiving objects:  71% (86/120)\rReceiving objects:  72% (87/120)\rReceiving objects:  73% (88/120)\rReceiving objects:  74% (89/120)\rReceiving objects:  75% (90/120)\rReceiving objects:  76% (92/120)\rReceiving objects:  77% (93/120)\rReceiving objects:  78% (94/120)\rReceiving objects:  79% (95/120)\rReceiving objects:  80% (96/120)\rReceiving objects:  81% (98/120)\rReceiving objects:  82% (99/120)\rReceiving objects:  83% (100/120)\rReceiving objects:  84% (101/120)\rReceiving objects:  85% (102/120)\rReceiving objects:  86% (104/120)\rReceiving objects:  87% (105/120)\rReceiving objects:  88% (106/120)\rReceiving objects:  89% (107/120)\rReceiving objects:  90% (108/120)\rReceiving objects:  91% (110/120)\rReceiving objects:  92% (111/120)\rReceiving objects:  93% (112/120)\rReceiving objects:  94% (113/120)\rReceiving objects:  95% (114/120)\rReceiving objects:  96% (116/120)\rReceiving objects:  97% (117/120)\rReceiving objects:  98% (118/120)\rReceiving objects:  99% (119/120)\rReceiving objects: 100% (120/120)\rReceiving objects: 100% (120/120), 222.76 KiB | 15.91 MiB/s, done.\n","Resolving deltas:   0% (0/53)\rResolving deltas:   1% (1/53)\rResolving deltas:   3% (2/53)\rResolving deltas:   5% (3/53)\rResolving deltas:   7% (4/53)\rResolving deltas:   9% (5/53)\rResolving deltas:  32% (17/53)\rResolving deltas:  33% (18/53)\rResolving deltas:  35% (19/53)\rResolving deltas:  45% (24/53)\rResolving deltas:  49% (26/53)\rResolving deltas:  56% (30/53)\rResolving deltas:  58% (31/53)\rResolving deltas:  60% (32/53)\rResolving deltas:  62% (33/53)\rResolving deltas:  67% (36/53)\rResolving deltas:  71% (38/53)\rResolving deltas:  73% (39/53)\rResolving deltas:  90% (48/53)\rResolving deltas:  96% (51/53)\rResolving deltas: 100% (53/53)\rResolving deltas: 100% (53/53), done.\n"]}]},{"cell_type":"code","metadata":{"id":"F-vYklhI5dab","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675913355661,"user_tz":-420,"elapsed":388,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"8d579bd0-1836-4bd2-e94a-591545f048c3"},"source":["cd /content/lightweight-human-pose-estimation.pytorch/"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/lightweight-human-pose-estimation.pytorch\n"]}]},{"cell_type":"code","metadata":{"id":"dRod9SOu77I6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675913361783,"user_tz":-420,"elapsed":2595,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"f648c4ef-1a5c-4c9d-fe24-0e9fd3fd5a75"},"source":["!wget https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-02-09 03:29:58--  https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n","Resolving download.01.org (download.01.org)... 23.66.220.114, 2600:1408:5400:38f::4b21, 2600:1408:5400:396::4b21\n","Connecting to download.01.org (download.01.org)|23.66.220.114|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 87959810 (84M) [application/octet-stream]\n","Saving to: ‘checkpoint_iter_370000.pth’\n","\n","checkpoint_iter_370 100%[===================>]  83.88M  58.9MB/s    in 1.4s    \n","\n","2023-02-09 03:30:00 (58.9 MB/s) - ‘checkpoint_iter_370000.pth’ saved [87959810/87959810]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"PdRcDXe38lHB","executionInfo":{"status":"ok","timestamp":1675913368119,"user_tz":-420,"elapsed":3745,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}}},"source":["import torch\n","import cv2\n","import numpy as np\n","from models.with_mobilenet import PoseEstimationWithMobileNet\n","from modules.keypoints import extract_keypoints, group_keypoints\n","from modules.load_state import load_state\n","from modules.pose import Pose, track_poses\n","import demo\n","\n","def get_rect(net, images, height_size):\n","    net = net.eval()\n","\n","    stride = 8\n","    upsample_ratio = 4\n","    num_keypoints = Pose.num_kpts\n","    previous_poses = []\n","    delay = 33\n","    for image in images:\n","        rect_path = image.replace('.%s' % (image.split('.')[-1]), '_rect.txt')\n","        img = cv2.imread(image, cv2.IMREAD_COLOR)\n","        orig_img = img.copy()\n","        orig_img = img.copy()\n","        heatmaps, pafs, scale, pad = demo.infer_fast(net, img, height_size, stride, upsample_ratio, cpu=False)\n","\n","        total_keypoints_num = 0\n","        all_keypoints_by_type = []\n","        for kpt_idx in range(num_keypoints):  # 19th for bg\n","            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n","\n","        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs)\n","        for kpt_id in range(all_keypoints.shape[0]):\n","            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n","            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n","        current_poses = []\n","\n","        rects = []\n","        for n in range(len(pose_entries)):\n","            if len(pose_entries[n]) == 0:\n","                continue\n","            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n","            valid_keypoints = []\n","            for kpt_id in range(num_keypoints):\n","                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n","                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n","                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n","                    valid_keypoints.append([pose_keypoints[kpt_id, 0], pose_keypoints[kpt_id, 1]])\n","            valid_keypoints = np.array(valid_keypoints)\n","            \n","            if pose_entries[n][10] != -1.0 or pose_entries[n][13] != -1.0:\n","              pmin = valid_keypoints.min(0)\n","              pmax = valid_keypoints.max(0)\n","\n","              center = (0.5 * (pmax[:2] + pmin[:2])).astype(np.int)\n","              radius = int(0.65 * max(pmax[0]-pmin[0], pmax[1]-pmin[1]))\n","            elif pose_entries[n][10] == -1.0 and pose_entries[n][13] == -1.0 and pose_entries[n][8] != -1.0 and pose_entries[n][11] != -1.0:\n","              # if leg is missing, use pelvis to get cropping\n","              center = (0.5 * (pose_keypoints[8] + pose_keypoints[11])).astype(np.int)\n","              radius = int(1.45*np.sqrt(((center[None,:] - valid_keypoints)**2).sum(1)).max(0))\n","              center[1] += int(0.05*radius)\n","            else:\n","              center = np.array([img.shape[1]//2,img.shape[0]//2])\n","              radius = max(img.shape[1]//2,img.shape[0]//2)\n","\n","            x1 = center[0] - radius\n","            y1 = center[1] - radius\n","\n","            rects.append([x1, y1, 2*radius, 2*radius])\n","\n","        np.savetxt(rect_path, np.array(rects), fmt='%d')"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"M6cGZD6f6IaY","executionInfo":{"status":"ok","timestamp":1675913381171,"user_tz":-420,"elapsed":11611,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}}},"source":["net = PoseEstimationWithMobileNet()\n","checkpoint = torch.load('checkpoint_iter_370000.pth', map_location='cpu')\n","load_state(net, checkpoint)\n","\n","get_rect(net.cuda(), [image_path], 512)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y0rgMInwTt0s"},"source":["## Download the Pretrained Model"]},{"cell_type":"code","metadata":{"id":"UrIcZweSNRFI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675913383590,"user_tz":-420,"elapsed":382,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"3137be0f-ee8a-427a-c125-83332064cdc4"},"source":["cd /content/pifuhd/"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/pifuhd\n"]}]},{"cell_type":"code","metadata":{"id":"k3jjm6HuQRk8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675913421469,"user_tz":-420,"elapsed":32424,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"14b5794e-ff61-4bd1-8228-1beaa1f4e9f3"},"source":["!sh ./scripts/download_trained_model.sh"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["+ mkdir -p checkpoints\n","+ cd checkpoints\n","+ wget https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt pifuhd.pt\n","--2023-02-09 03:30:28--  https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1548375177 (1.4G) [application/octet-stream]\n","Saving to: ‘pifuhd.pt’\n","\n","pifuhd.pt           100%[===================>]   1.44G  48.7MB/s    in 32s     \n","\n","2023-02-09 03:31:00 (46.7 MB/s) - ‘pifuhd.pt’ saved [1548375177/1548375177]\n","\n","--2023-02-09 03:31:00--  http://pifuhd.pt/\n","Resolving pifuhd.pt (pifuhd.pt)... failed: Name or service not known.\n","wget: unable to resolve host address ‘pifuhd.pt’\n","FINISHED --2023-02-09 03:31:00--\n","Total wall clock time: 32s\n","Downloaded: 1 files, 1.4G in 32s (46.7 MB/s)\n"]}]},{"cell_type":"markdown","metadata":{"id":"6heKcA-0QEBw"},"source":["## Run PIFuHD!\n"]},{"cell_type":"code","metadata":{"id":"5995t2PnQTmG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675913459006,"user_tz":-420,"elapsed":23286,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"31585dfa-2223-40f9-8f2c-af681381b20f"},"source":["# Warning: all images with the corresponding rectangle files under -i will be processed. \n","!python -m apps.simple_test -r 256 --use_rect -i $image_dir\n","\n","# seems that 256 is the maximum resolution that can fit into Google Colab. \n","# If you want to reconstruct a higher-resolution mesh, please try with your own machine. "],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Resuming from  ./checkpoints/pifuhd.pt\n","Warning: opt is overwritten.\n","test data size:  1\n","initialize network with normal\n","initialize network with normal\n","generate mesh (test) ...\n","  0% 0/1 [00:00<?, ?it/s]./results/pifuhd_final/recon/result_ssssssssss_256.obj\n","/content/pifuhd/lib/mesh_util.py:77: FutureWarning: marching_cubes_lewiner is deprecated in favor of marching_cubes. marching_cubes_lewiner will be removed in version 0.19\n","  verts, faces, normals, values = measure.marching_cubes_lewiner(sdf, thresh)\n","100% 1/1 [00:09<00:00,  9.29s/it]\n"]}]},{"cell_type":"markdown","metadata":{"id":"EUZ8Nt5rNFXZ"},"source":["## Render the result"]},{"cell_type":"code","metadata":{"id":"5xp5s5uiOiDv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675913478575,"user_tz":-420,"elapsed":8636,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"6e39f164-8cf0-4cdd-fc6b-99997c2cc6d5"},"source":["!pip install pytorch3d\n","\n","# If you get an error in the next cell, you can instead try the following command (don't forget to comment out the one above!).\n","# Note that this error is caused by inconsistent cuda version for the pytorch3d package and pytorch in Colab environment.\n","# Thus, this issue may persist unless pytorch3d in the pip package is updated with the cuda version consistent with pytorch in Colab.\n","# Also please be aware that the following command is much slower as it builds pytorch3d from scratch.\n","\n","# !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\n","\n","# You can try another solution below as well. This is also slow and requires you to restart the runtime.\n","\n","# !pip install 'torch==1.6.0+cu101' -f https://download.pytorch.org/whl/torch_stable.html\n","# !pip install 'torchvision==0.7.0+cu101' -f https://download.pytorch.org/whl/torch_stable.html\n","# !pip install 'pytorch3d==0.2.5'"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch3d\n","  Downloading pytorch3d-0.3.0-cp38-cp38-manylinux1_x86_64.whl (30.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.0/30.0 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fvcore\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.8/dist-packages (from pytorch3d) (0.14.1+cu116)\n","Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.4->pytorch3d) (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.4->pytorch3d) (4.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.4->pytorch3d) (2.25.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.4->pytorch3d) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.4->pytorch3d) (1.21.6)\n","Collecting yacs>=0.1.6\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorch3d) (6.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorch3d) (4.64.1)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorch3d) (2.2.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorch3d) (0.8.10)\n","Collecting iopath>=0.1.7\n","  Downloading iopath-0.1.10.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting portalocker\n","  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.4->pytorch3d) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.4->pytorch3d) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.4->pytorch3d) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.4->pytorch3d) (2022.12.7)\n","Building wheels for collected packages: fvcore, iopath\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61431 sha256=dd5b671359706f372c2ff144e0bdc20c39485ba8d2f016b6546ddc23ff9ede0a\n","  Stored in directory: /root/.cache/pip/wheels/b8/79/07/c0e9367f5b5ea325e246bd73651e8af175fabbef943043b1cc\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31547 sha256=8c5b92d291cf6cdc03f29639e982665eed3bbf418809a495d125d84071605bfb\n","  Stored in directory: /root/.cache/pip/wheels/89/3e/24/0f349c0b2eeb6965903035f3b00dbb5c9bea437b4a2f18d82c\n","Successfully built fvcore iopath\n","Installing collected packages: yacs, portalocker, iopath, fvcore, pytorch3d\n","Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.7.0 pytorch3d-0.3.0 yacs-0.1.8\n"]}]},{"cell_type":"code","metadata":{"id":"afwL_-ROCmDf","colab":{"base_uri":"https://localhost:8080/","height":520},"executionInfo":{"status":"error","timestamp":1675913995575,"user_tz":-420,"elapsed":451,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"4c9c70ce-d9a9-4d76-c29c-350926bd2859"},"source":["from lib.colab_util import generate_video_from_obj, set_renderer, video\n","\n","renderer = set_renderer()\n","generate_video_from_obj(obj_path, out_img_path, video_path, renderer)\n","\n","# we cannot play a mp4 video generated by cv2\n","!ffmpeg -i $video_path -vcodec libx264 $video_display_path -y -loglevel quiet\n","video(video_display_path)"],"execution_count":22,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-22614b7ef849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_video_from_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_renderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgenerate_video_from_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_img_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/pifuhd/lib/colab_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Util function for loading meshes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_objs_as_meshes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch3d/io/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobj_io\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_objs_as_meshes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mply_io\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_ply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_ply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch3d/io/obj_io.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtl_io\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_mtl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_mesh_texture_atlas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_check_faces_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_make_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTexturesAtlas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTexturesUV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeshes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_meshes_as_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch3d/renderer/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m from .blending import (\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mBlendParams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhard_rgb_blend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch3d/renderer/blending.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# pyre-fixme[21]: Could not find name `_C` in `pytorch3d`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: libcudart.so.10.1: cannot open shared object file: No such file or directory","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","metadata":{"id":"eUEXAvcvkVYV"},"source":["## Tips for Inputs: My results are broken!\n","\n","(Kudos to those who share results on twitter with [#pifuhd](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live) tag!!!!)\n","\n","Due to the limited variation in the training data, your results might be broken sometimes. Here I share some useful tips to get resonable results. \n","\n","*   Use high-res image. The model is trained with 1024x1024 images. Use at least 512x512 with fine-details. Low-res images and JPEG artifacts may result in unsatisfactory results. \n","*   Use an image with a single person. If the image contain multiple people, reconstruction quality is likely degraded.\n","*   Front facing with standing works best (or with fashion pose)\n","*   The entire body is covered within the image. (Note: now missing legs is partially supported)\n","*   Make sure the input image is well lit. Exteremy dark or bright image and strong shadow often create artifacts.\n","*   I recommend nearly parallel camera angle to the ground. High camera height may result in distorted legs or high heels. \n","*   If the background is cluttered, use less complex background or try removing it using https://www.remove.bg/ before processing.\n","*   It's trained with human only. Anime characters may not work well (To my surprise, indeed many people tried it!!).\n","*   Search on twitter with [#pifuhd](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live) tag to get a better sense of what succeeds and what fails. \n"]},{"cell_type":"markdown","metadata":{"id":"u6U0K5CNAO_u"},"source":["## Share your result! \n","Please share your results with[ #pifuhd](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live) tag on Twitter. Sharing your good/bad results helps and encourages the authors to further push towards producition-quality human digitization at home.\n","**As the tweet buttom below doesn't add the result video automatically, please download the result video above and manually add it to the tweet.**"]},{"cell_type":"code","metadata":{"id":"1CBxbdrM9F-9","colab":{"base_uri":"https://localhost:8080/","height":49},"executionInfo":{"status":"ok","timestamp":1592786011679,"user_tz":-540,"elapsed":1908,"user":{"displayName":"Shunsuke Saito","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgjRdEU7uvjC7jQl8KzsEC_J-vZx7aCl-esOpki=s64","userId":"17785988887781081181"}},"outputId":"066da2e3-4934-4133-d9fc-525365b49176"},"source":["import IPython\n","IPython.display.HTML('<a href=\"https://twitter.com/intent/tweet?button_hashtag=pifuhd&ref_src=twsrc%5Etfw\" class=\"twitter-hashtag-button\" data-size=\"large\" data-text=\"Google Colab Link: \" data-url=\"https://bit.ly/37sfogZ\" data-show-count=\"false\">Tweet #pifuhd</a><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  (Don\\'t forget to add your result to the tweet!)')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<a href=\"https://twitter.com/intent/tweet?button_hashtag=pifuhd&ref_src=twsrc%5Etfw\" class=\"twitter-hashtag-button\" data-size=\"large\" data-text=\"Google Colab Link: \" data-url=\"https://bit.ly/37sfogZ\" data-show-count=\"false\">Tweet #pifuhd</a><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  (Don't forget to add your result to the tweet!)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"2d-1pR8UR7PR"},"source":["## Cool Applications\n","Special thanks to those who play with PIFuHD and came up with many creative applications!! If you made any cool applications, please tweet your demo with [#pifuhd](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live). I'm constantly checking results there.\n","If you need complete texture on the mesh, please try my previous work [PIFu](https://github.com/shunsukesaito/PIFu) as well! It supports 3D reconstruction + texturing from a single image although the geometry quality may not be as good as PIFuHD."]},{"cell_type":"code","metadata":{"id":"68JDAYJFSFMV","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1675913518988,"user_tz":-420,"elapsed":432,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"5473d6c8-e4d3-4ac1-ad98-2f879d7e0ac5"},"source":["IPython.display.HTML('<h2>Rigging (Mixamo) + Photoreal Rendering (Blender)</h2><blockquote class=\"twitter-tweet\"><p lang=\"pt\" dir=\"ltr\">vcs ainda tem a PACHORRA de me dizer que eu não sei dançar<a href=\"https://twitter.com/hashtag/b3d?src=hash&amp;ref_src=twsrc%5Etfw\">#b3d</a> <a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> <a href=\"https://t.co/kHCnLh6zxH\">pic.twitter.com/kHCnLh6zxH</a></p>&mdash; lukas arendero (@lukazvd) <a href=\"https://twitter.com/lukazvd/status/1274810484798128131?ref_src=twsrc%5Etfw\">June 21, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>FaceApp + Rigging (Mixamo)</h2><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\">カツラかぶってる自分に見える <a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> <a href=\"https://t.co/V8o7VduTiG\">pic.twitter.com/V8o7VduTiG</a></p>&mdash; Shuhei Tsuchida (@shuhei2306) <a href=\"https://twitter.com/shuhei2306/status/1274507242910314498?ref_src=twsrc%5Etfw\">June 21, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>Rigging (Mixamo) + AR (Adobe Aero)</AR><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\">写真→PIFuHD→Mixamo→AdobeAeroでサウンド付きARを作成。Zip化してLINEでARコンテンツを共有。<br>写真が1枚あれば簡単にARの3Dアニメーションが作れる時代…凄い。<a href=\"https://twitter.com/hashtag/PIFuHD?src=hash&amp;ref_src=twsrc%5Etfw\">#PIFuHD</a> <a href=\"https://twitter.com/hashtag/AdobeAero?src=hash&amp;ref_src=twsrc%5Etfw\">#AdobeAero</a> <a href=\"https://twitter.com/hashtag/Mixamo?src=hash&amp;ref_src=twsrc%5Etfw\">#Mixamo</a> <a href=\"https://t.co/CbiMi4gZ0K\">pic.twitter.com/CbiMi4gZ0K</a></p>&mdash; モジョン (@mojon1) <a href=\"https://twitter.com/mojon1/status/1273217947872317441?ref_src=twsrc%5Etfw\">June 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>3D Printing</h2><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> 楽しい〜<br>小さい自分プリントした <a href=\"https://t.co/4qyWuij0Hs\">pic.twitter.com/4qyWuij0Hs</a></p>&mdash; isb (@vxzxzxzxv) <a href=\"https://twitter.com/vxzxzxzxv/status/1273136266406694913?ref_src=twsrc%5Etfw\">June 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>')"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<h2>Rigging (Mixamo) + Photoreal Rendering (Blender)</h2><blockquote class=\"twitter-tweet\"><p lang=\"pt\" dir=\"ltr\">vcs ainda tem a PACHORRA de me dizer que eu não sei dançar<a href=\"https://twitter.com/hashtag/b3d?src=hash&amp;ref_src=twsrc%5Etfw\">#b3d</a> <a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> <a href=\"https://t.co/kHCnLh6zxH\">pic.twitter.com/kHCnLh6zxH</a></p>&mdash; lukas arendero (@lukazvd) <a href=\"https://twitter.com/lukazvd/status/1274810484798128131?ref_src=twsrc%5Etfw\">June 21, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>FaceApp + Rigging (Mixamo)</h2><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\">カツラかぶってる自分に見える <a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> <a href=\"https://t.co/V8o7VduTiG\">pic.twitter.com/V8o7VduTiG</a></p>&mdash; Shuhei Tsuchida (@shuhei2306) <a href=\"https://twitter.com/shuhei2306/status/1274507242910314498?ref_src=twsrc%5Etfw\">June 21, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>Rigging (Mixamo) + AR (Adobe Aero)</AR><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\">写真→PIFuHD→Mixamo→AdobeAeroでサウンド付きARを作成。Zip化してLINEでARコンテンツを共有。<br>写真が1枚あれば簡単にARの3Dアニメーションが作れる時代…凄い。<a href=\"https://twitter.com/hashtag/PIFuHD?src=hash&amp;ref_src=twsrc%5Etfw\">#PIFuHD</a> <a href=\"https://twitter.com/hashtag/AdobeAero?src=hash&amp;ref_src=twsrc%5Etfw\">#AdobeAero</a> <a href=\"https://twitter.com/hashtag/Mixamo?src=hash&amp;ref_src=twsrc%5Etfw\">#Mixamo</a> <a href=\"https://t.co/CbiMi4gZ0K\">pic.twitter.com/CbiMi4gZ0K</a></p>&mdash; モジョン (@mojon1) <a href=\"https://twitter.com/mojon1/status/1273217947872317441?ref_src=twsrc%5Etfw\">June 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>3D Printing</h2><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> 楽しい〜<br>小さい自分プリントした <a href=\"https://t.co/4qyWuij0Hs\">pic.twitter.com/4qyWuij0Hs</a></p>&mdash; isb (@vxzxzxzxv) <a href=\"https://twitter.com/vxzxzxzxv/status/1273136266406694913?ref_src=twsrc%5Etfw\">June 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"lX5CTTW_KWhQ"},"source":[],"execution_count":null,"outputs":[]}]}