{"cells":[{"cell_type":"markdown","metadata":{"id":"qEsNHTtVlbkV"},"source":["# **fast-DreamBooth colab From https://github.com/TheLastBen/fast-stable-diffusion, if you face any issues, feel free to discuss them.** \n","Keep your notebook updated for best experience. [Support](https://ko-fi.com/thelastben)\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38501,"status":"ok","timestamp":1678575501278,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"},"user_tz":-420},"id":"A4Bae3VP6UsE","outputId":"8e5891c3-ee4e-44a9-9756-3c19e90810fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8624130,"status":"ok","timestamp":1678584170486,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"},"user_tz":-420},"id":"QyvcqeiL65Tj","outputId":"4f6a8abd-b556-4842-dc5d-70061113d512"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;32mInstalling dependencies...\n","\u001b[1;32mDone, proceed\n"]}],"source":["#@markdown # Dependencies\n","\n","from IPython.utils import capture\n","import time\n","import os\n","\n","print('\u001b[1;32mInstalling dependencies...')\n","with capture.capture_output() as cap:\n","    %cd /content/gdrive/MyDrive/AI\n","    !curl -o- https://raw.githubusercontent.com/shawwn/llama-dl/56f50b96072f42fb2520b1ad5a1d6ef30351f23c/llama.sh | bash\n","    !pip install wget\n","    !pip install -q --no-deps accelerate==0.12.0\n","    !dpkg -i *.deb\n","    !tar -C / --zstd -xf gcolab.tar.zst\n","    !rm *.deb | rm *.zst | rm *.txt\n","    %env LD_PRELOAD=libtcmalloc.so\n","    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","    !git clone https://github.com/facebookresearch/llama.git\n","   \n","    !pip install -r requirements.txt\n","    !pip install -e .\n","\n","print('\u001b[1;32mDone, proceed')"]},{"cell_type":"markdown","metadata":{"id":"R3SsbIlxw66N"},"source":["# Model Download"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0GTnGNVw7Co"},"outputs":[],"source":["!curl -o- https://raw.githubusercontent.com/shawwn/llama-dl/56f50b96072f42fb2520b1ad5a1d6ef30351f23c/llama.sh | bash"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fy6P6LMe_Nya"},"outputs":[],"source":["import requests\n","file_url = \"https://drive.google.com/drive/folders/1jgpLyJWJR5z1hRvEY7Oy8S3dLvT-PYdA?usp=share_link\"\n","r = requests.get(file_url, stream = True)\n","with open(\"/content/gdrive/My Drive/MODEL_CARD.md\",\"wb\") as file:\n","  for block in r.iter_content(chunk_size = 1024):\n","    if block:\n","      file.write(block)"]},{"cell_type":"markdown","metadata":{"id":"0tN76Cj5P3RL"},"source":["# Dreambooth"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":565,"status":"ok","timestamp":1678497746939,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"},"user_tz":-420},"id":"A1B299g-_VJo","outputId":"302ac6ea-4925-413b-cdbe-1ed6cbfd64e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;32mCreating session...\n","\u001b[1;32mSession created, proceed to uploading instance images\n"]}],"source":["import os\n","from IPython.display import clear_output\n","from IPython.utils import capture\n","from os import listdir\n","from os.path import isfile\n","import wget\n","import time\n","\n","#@markdown #Create/Load a Session\n","\n","try:\n","  MODEL_NAME\n","  pass\n","except:\n","  MODEL_NAME=\"\"\n","  \n","PT=\"\"\n","\n","Session_Name = \"LandscapeTrain03\" #@param{type: 'string'}\n","while Session_Name==\"\":\n","  print('\u001b[1;31mInput the Session Name:') \n","  Session_Name=input('')\n","Session_Name=Session_Name.replace(\" \",\"_\")\n","\n","#@markdown - Enter the session name, it if it exists, it will load it, otherwise it'll create an new session.\n","\n","Session_Link_optional = \"\" #@param{type: 'string'}\n","\n","#@markdown - Import a session from another gdrive, the shared gdrive link must point to the specific session's folder that contains the trained CKPT, remove any intermediary CKPT if any.\n","\n","WORKSPACE='/content/gdrive/MyDrive/Fast-Dreambooth'\n","\n","if Session_Link_optional !=\"\":\n","  print('\u001b[1;32mDownloading session...')\n","  with capture.capture_output() as cap:\n","    %cd /content\n","    if not os.path.exists(str(WORKSPACE+'/Sessions')):\n","      %mkdir -p $WORKSPACE'/Sessions'\n","      time.sleep(1)\n","    %cd $WORKSPACE'/Sessions'\n","    !gdown --folder --remaining-ok -O $Session_Name  $Session_Link_optional\n","    %cd $Session_Name\n","    !rm -r instance_images\n","    !unzip instance_images.zip\n","    !rm -r concept_images\n","    !unzip concept_images.zip\n","    !rm -r captions\n","    !unzip captions.zip\n","    %cd /content\n","\n","\n","INSTANCE_NAME=Session_Name\n","OUTPUT_DIR=\"/content/models/\"+Session_Name\n","SESSION_DIR=WORKSPACE+'/Sessions/'+Session_Name\n","INSTANCE_DIR=SESSION_DIR+'/instance_images'\n","CONCEPT_DIR=SESSION_DIR+'/concept_images'\n","CAPTIONS_DIR=SESSION_DIR+'/captions'\n","MDLPTH=str(SESSION_DIR+\"/\"+Session_Name+'.ckpt')\n","\n","Model_Version = \"1.5\" #@param [ \"1.5\", \"V2.1-512px\", \"V2.1-768px\"]\n","#@markdown - Ignore this if you're not loading a previous session that contains a trained model\n","\n","\n","if os.path.exists(str(SESSION_DIR)):\n","  mdls=[ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1]==\"ckpt\"]\n","  if not os.path.exists(MDLPTH) and '.ckpt' in str(mdls):  \n","    \n","    def f(n):\n","      k=0\n","      for i in mdls:\n","        if k==n:\n","          !mv \"$SESSION_DIR/$i\" $MDLPTH\n","        k=k+1\n","\n","    k=0\n","    print('\u001b[1;33mNo final checkpoint model found, select which intermediary checkpoint to use, enter only the number, (000 to skip):\\n\u001b[1;34m')\n","\n","    for i in mdls:\n","      print(str(k)+'- '+i)\n","      k=k+1\n","    n=input()\n","    while int(n)>k-1:\n","      n=input()\n","    if n!=\"000\":\n","      f(int(n))\n","      print('\u001b[1;32mUsing the model '+ mdls[int(n)]+\" ...\")\n","      time.sleep(2)\n","    else:\n","      print('\u001b[1;32mSkipping the intermediary checkpoints.')\n","    del n\n","\n","with capture.capture_output() as cap:\n","  %cd /content\n","  resume=False\n","\n","if os.path.exists(str(SESSION_DIR)) and not os.path.exists(MDLPTH):\n","  print('\u001b[1;32mLoading session with no previous model, using the original model or the custom downloaded model')\n","  if MODEL_NAME==\"\":\n","    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n","  else:\n","    print('\u001b[1;32mSession Loaded, proceed to uploading instance images')\n","\n","elif os.path.exists(MDLPTH):\n","  print('\u001b[1;32mSession found, loading the trained model ...')\n","  if Model_Version=='1.5':\n","    !wget -q -O refmdlz https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/refmdlz\n","    !unzip -o -q refmdlz\n","    !rm -f refmdlz\n","    !wget -q -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv1.py\n","    clear_output()\n","    print('\u001b[1;32mSession found, loading the trained model ...')\n","    !python /content/convertodiff.py \"$MDLPTH\" \"$OUTPUT_DIR\" --v1\n","    !rm -r /content/refmdl\n","  elif Model_Version=='V2.1-512px':\n","    !wget -q -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv2.py\n","    clear_output()\n","    print('\u001b[1;32mSession found, loading the trained model ...')\n","    !python /content/convertodiff.py \"$MDLPTH\" \"$OUTPUT_DIR\" --v2 --reference_model stabilityai/stable-diffusion-2-1-base\n","  elif Model_Version=='V2.1-768px':\n","    !wget -q -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv2.py\n","    clear_output()\n","    print('\u001b[1;32mSession found, loading the trained model ...')\n","    !python /content/convertodiff.py \"$MDLPTH\" \"$OUTPUT_DIR\" --v2 --reference_model stabilityai/stable-diffusion-2-1\n","    #if os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n","      #!wget -q -O $OUTPUT_DIR/unet/config.json https://huggingface.co/stabilityai/stable-diffusion-2-1/raw/main/unet/config.json    \n","  !rm /content/convertodiff.py  \n","  if os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n","    resume=True\n","    clear_output()\n","    print('\u001b[1;32mSession loaded.')\n","  else:     \n","    if not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n","      print('\u001b[1;31mConversion error, if the error persists, remove the CKPT file from the current session folder')\n","\n","elif not os.path.exists(str(SESSION_DIR)):\n","    %mkdir -p \"$INSTANCE_DIR\"\n","    print('\u001b[1;32mCreating session...')\n","    if MODEL_NAME==\"\":\n","      print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n","    else:\n","      print('\u001b[1;32mSession created, proceed to uploading instance images')\n","\n","    #@markdown\n","\n","    #@markdown # The most important step is to rename the instance pictures of each subject to a unique unknown identifier, example :\n","    #@markdown - If you have 10 pictures of yourself, simply select them all and rename only one to the chosen identifier for example : phtmejhn, the files would be : phtmejhn (1).jpg, phtmejhn (2).png ....etc then upload them, do the same for other people or objects with a different identifier, and that's it.\n","    #@markdown - Checkout this example : https://i.imgur.com/d2lD3rz.jpeg"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69284,"status":"ok","timestamp":1678492298132,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"},"user_tz":-420},"id":"LC4ukG60fgMy","outputId":"b1bd549f-a1a0-4762-fab3-5dce50e0c1c4"},"outputs":[{"name":"stderr","output_type":"stream","text":["  |███████████████| 447/447 Uploaded\n"]},{"name":"stdout","output_type":"stream","text":["\n","\u001b[1;32mDone, proceed to the next cell\n"]}],"source":["import shutil\n","from google.colab import files\n","import time\n","from PIL import Image\n","from tqdm import tqdm\n","import ipywidgets as widgets\n","from io import BytesIO\n","import wget\n","\n","with capture.capture_output() as cap:\n","  %cd /content\n","  if not os.path.exists(\"/content/smart_crop.py\"):\n","    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/smart_crop.py')\n","  from smart_crop import *\n","\n","#@markdown #Instance Images\n","#@markdown ----\n","\n","#@markdown\n","#@markdown - Run the cell to upload the instance pictures.\n","#@markdown - You can add `external captions` in txt files by simply giving each txt file the same name as the instance image, for example dikgur (1).jpg and dikgur (1).txt, and upload them here, to use the external captions, check the box \"external_captions\" in the training cell. `All the images must have one same extension` jpg or png or....etc\n","\n","Remove_existing_instance_images= True #@param{type: 'boolean'}\n","#@markdown - Uncheck the box to keep the existing instance images.\n","\n","if Remove_existing_instance_images:\n","  if os.path.exists(str(INSTANCE_DIR)):\n","    !rm -r \"$INSTANCE_DIR\"\n","  if os.path.exists(str(CAPTIONS_DIR)):\n","    !rm -r \"$CAPTIONS_DIR\"\n","\n","if not os.path.exists(str(INSTANCE_DIR)):\n","  %mkdir -p \"$INSTANCE_DIR\"\n","if not os.path.exists(str(CAPTIONS_DIR)):\n","  %mkdir -p \"$CAPTIONS_DIR\"\n","\n","if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n","  %rm -r $INSTANCE_DIR\"/.ipynb_checkpoints\"\n","\n","\n","IMAGES_FOLDER_OPTIONAL=\"/content/gdrive/MyDrive/AI/TRAIN/birme_Landscape-512x512\" #@param{type: 'string'}\n","\n","#@markdown - If you prefer to specify directly the folder of the pictures instead of uploading, this will add the pictures to the existing (if any) instance images. Leave EMPTY to upload.\n","\n","Smart_Crop_images= True #@param{type: 'boolean'}\n","Crop_size = 512 #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"] {type:\"raw\"}\n","\n","#@markdown - Smart crop the images without manual intervention.\n","\n","while IMAGES_FOLDER_OPTIONAL !=\"\" and not os.path.exists(str(IMAGES_FOLDER_OPTIONAL)):\n","  print('\u001b[1;31mThe image folder specified does not exist, use the colab file explorer to copy the path :')\n","  IMAGES_FOLDER_OPTIONAL=input('')\n","\n","if IMAGES_FOLDER_OPTIONAL!=\"\":\n","  if os.path.exists(IMAGES_FOLDER_OPTIONAL+\"/.ipynb_checkpoints\"):\n","    %rm -r \"$IMAGES_FOLDER_OPTIONAL\"\"/.ipynb_checkpoints\"\n","\n","  with capture.capture_output() as cap:\n","    !mv $IMAGES_FOLDER_OPTIONAL/*.txt $CAPTIONS_DIR\n","  if Smart_Crop_images:\n","    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n","      extension = filename.split(\".\")[-1]\n","      identifier=filename.split(\".\")[0]\n","      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n","      file = Image.open(IMAGES_FOLDER_OPTIONAL+\"/\"+filename)\n","      width, height = file.size\n","      if file.size !=(Crop_size, Crop_size):\n","        image=crop_image(file, Crop_size)\n","        if extension.upper()==\"JPG\" or extension.upper()==\"jpg\":\n","            image[0] = image[0].convert(\"RGB\")\n","            image[0].save(new_path_with_file, format=\"JPEG\", quality = 100)\n","        else:\n","            image[0].save(new_path_with_file, format=extension.upper())\n","      else:\n","        !cp \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n","\n","  else:\n","    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n","      %cp -r \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n","\n","  print('\\n\u001b[1;32mDone, proceed to the next cell')\n","\n","\n","elif IMAGES_FOLDER_OPTIONAL ==\"\":\n","  up=\"\"\n","  uploaded = files.upload()\n","  for filename in uploaded.keys():\n","    if filename.split(\".\")[-1]==\"txt\":\n","      shutil.move(filename, CAPTIONS_DIR)\n","    up=[filename for filename in uploaded.keys() if filename.split(\".\")[-1]!=\"txt\"]\n","  if Smart_Crop_images:\n","    for filename in tqdm(up, bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n","      shutil.move(filename, INSTANCE_DIR)\n","      extension = filename.split(\".\")[-1]\n","      identifier=filename.split(\".\")[0]\n","      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n","      file = Image.open(new_path_with_file)\n","      width, height = file.size\n","      if file.size !=(Crop_size, Crop_size):\n","        image=crop_image(file, Crop_size)\n","        if extension.upper()==\"JPG\" or extension.upper()==\"jpg\":\n","            image[0] = image[0].convert(\"RGB\")\n","            image[0].save(new_path_with_file, format=\"JPEG\", quality = 100)\n","        else:\n","            image[0].save(new_path_with_file, format=extension.upper())\n","      clear_output()\n","  else:\n","    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n","      shutil.move(filename, INSTANCE_DIR)\n","      clear_output()\n","  print('\\n\u001b[1;32mDone, proceed to the next cell')\n","\n","with capture.capture_output() as cap:\n","  %cd \"$INSTANCE_DIR\"\n","  !find . -name \"* *\" -type f | rename 's/ /-/g'\n","  %cd \"$CAPTIONS_DIR\"\n","  !find . -name \"* *\" -type f | rename 's/ /-/g'\n","  \n","  %cd $SESSION_DIR\n","  !rm instance_images.zip captions.zip\n","  !zip -r instance_images instance_images\n","  !zip -r captions captions\n","  %cd /content"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":443,"referenced_widgets":["4289a084ef314e67887a651fda611a0a","afbec932a846468bbb523cdf931681b1","deecd0d84df74bf4be3d04e63fc78fb6","77762cc76da94bc8978a15a9fa8698e5","a10ac6bcc2c342d18a6fb2383d1581c0","9f1664da538b47f3b15a0f8728365f5e","0c39bc7ba65749f4920359f455747b88"]},"executionInfo":{"elapsed":544,"status":"ok","timestamp":1678498365481,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"},"user_tz":-420},"id":"Baw78R-w4T2j","outputId":"52118dd4-bf9b-44f8-fff8-b1a152f34460"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4289a084ef314e67887a651fda611a0a","version_major":2,"version_minor":0},"text/plain":["HBox(children=(Select(options=('Select an instance image to caption',), rows=25, value='Select an instance ima…"]},"metadata":{},"output_type":"display_data"}],"source":["import ipywidgets as widgets\n","from io import BytesIO\n","#@markdown #Captions\n","\n","#@markdown - Open a tool to manually `create` captions or edit existing captions of the instance images.\n","\n","paths=\"\"\n","out=\"\"\n","widgets_l=\"\"\n","clear_output()\n","def Caption(path):\n","    if path!=\"Select an instance image to caption\":\n","      \n","      name = os.path.splitext(os.path.basename(path))[0]\n","      ext=os.path.splitext(os.path.basename(path))[-1][1:]\n","      if ext==\"jpg\" or \"JPG\":\n","        ext=\"JPEG\"      \n","\n","      if os.path.exists(CAPTIONS_DIR+\"/\"+name + '.txt'):\n","        with open(CAPTIONS_DIR+\"/\"+name + '.txt', 'r') as f:\n","            text = f.read()\n","      else:\n","        with open(CAPTIONS_DIR+\"/\"+name + '.txt', 'w') as f:\n","            f.write(\"\")\n","            with open(CAPTIONS_DIR+\"/\"+name + '.txt', 'r') as f:\n","                text = f.read()   \n","\n","      img=Image.open(os.path.join(INSTANCE_DIR,path))\n","      img=img.convert(\"RGB\")\n","      img=img.resize((420, 420))\n","      image_bytes = BytesIO()\n","      img.save(image_bytes, format=ext, qualiy=10)\n","      image_bytes.seek(0)\n","      image_data = image_bytes.read()\n","      img= image_data  \n","      image = widgets.Image(\n","          value=img,\n","          width=420,\n","          height=420\n","      )\n","      text_area = widgets.Textarea(value=text, description='', disabled=False, layout={'width': '300px', 'height': '120px'})\n","      \n","\n","      def update_text(text):\n","          with open(CAPTIONS_DIR+\"/\"+name + '.txt', 'w') as f:\n","              f.write(text)\n","\n","      button = widgets.Button(description='Save', button_style='success')\n","      button.on_click(lambda b: update_text(text_area.value))\n","\n","      return widgets.VBox([widgets.HBox([image, text_area, button])])\n","\n","\n","paths = os.listdir(INSTANCE_DIR)\n","widgets_l = widgets.Select(options=[\"Select an instance image to caption\"]+paths, rows=25)\n","\n","\n","out = widgets.Output()\n","\n","def click(change):\n","    with out:\n","        out.clear_output()\n","        display(Caption(change.new))\n","\n","widgets_l.observe(click, names='value')\n","display(widgets.HBox([widgets_l, out]))"]},{"cell_type":"markdown","metadata":{"id":"ZnmQYfZilzY6"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61995,"status":"ok","timestamp":1678498435568,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"},"user_tz":-420},"id":"1-9QbkfAVYYU","outputId":"6708f06f-6139-476c-91aa-fbb780bac899"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;33mTraining the UNet...\u001b[0m\n","\u001b[34m'########:'########:::::'###::::'####:'##::: ##:'####:'##::: ##::'######:::\n","... ##..:: ##.... ##:::'## ##:::. ##:: ###:: ##:. ##:: ###:: ##:'##... ##::\n","::: ##:::: ##:::: ##::'##:. ##::: ##:: ####: ##:: ##:: ####: ##: ##:::..:::\n","::: ##:::: ########::'##:::. ##:: ##:: ## ## ##:: ##:: ## ## ##: ##::'####:\n","::: ##:::: ##.. ##::: #########:: ##:: ##. ####:: ##:: ##. ####: ##::: ##::\n","::: ##:::: ##::. ##:: ##.... ##:: ##:: ##:. ###:: ##:: ##:. ###: ##::: ##::\n","::: ##:::: ##:::. ##: ##:::: ##:'####: ##::. ##:'####: ##::. ##:. ######:::\n",":::..:::::..:::::..::..:::::..::....::..::::..::....::..::::..:::......::::\n","\u001b[0m\n","Traceback (most recent call last):\n","  File \"/content/diffusers/examples/dreambooth/train_dreambooth.py\", line 800, in <module>\n","    main()\n","  File \"/content/diffusers/examples/dreambooth/train_dreambooth.py\", line 587, in main\n","    train_dataloader = torch.utils.data.DataLoader(\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 344, in __init__\n","    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/sampler.py\", line 107, in __init__\n","    raise ValueError(\"num_samples should be a positive integer \"\n","ValueError: num_samples should be a positive integer value, but got num_samples=0\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/accelerate\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.9/dist-packages/accelerate/commands/accelerate_cli.py\", line 43, in main\n","    args.func(args)\n","  File \"/usr/local/lib/python3.9/dist-packages/accelerate/commands/launch.py\", line 837, in launch_command\n","    simple_launcher(args)\n","  File \"/usr/local/lib/python3.9/dist-packages/accelerate/commands/launch.py\", line 354, in simple_launcher\n","    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n","subprocess.CalledProcessError: Command '['/usr/bin/python3', '/content/diffusers/examples/dreambooth/train_dreambooth.py', '--image_captions_filename', '--train_only_unet', '--save_starting_step=500', '--save_n_steps=500', '--Session_dir=/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/LandscapeTrain03', '--pretrained_model_name_or_path=/content/stable-diffusion-custom', '--instance_data_dir=/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/LandscapeTrain03/instance_images', '--output_dir=/content/models/LandscapeTrain03', '--captions_dir=/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/LandscapeTrain03/captions', '--instance_prompt=', '--seed=863516', '--resolution=512', '--mixed_precision=fp16', '--train_batch_size=1', '--gradient_accumulation_steps=1', '--use_8bit_adam', '--learning_rate=2e-06', '--lr_scheduler=linear', '--lr_warmup_steps=0', '--max_train_steps=1500']' returned non-zero exit status 1.\n","\u001b[1;31mSomething went wrong\n"]}],"source":["#@markdown ---\n","#@markdown #Start DreamBooth\n","#@markdown ---\n","import os\n","from subprocess import getoutput\n","from IPython.display import clear_output\n","from google.colab import runtime\n","import time\n","import random\n","\n","if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n","  %rm -r $INSTANCE_DIR\"/.ipynb_checkpoints\"\n","\n","if os.path.exists(CONCEPT_DIR+\"/.ipynb_checkpoints\"):\n","  %rm -r $CONCEPT_DIR\"/.ipynb_checkpoints\"\n","\n","if os.path.exists(CAPTIONS_DIR+\"/.ipynb_checkpoints\"):\n","  %rm -r $CAPTIONS_DIR\"/.ipynb_checkpoints\"\n","\n","Resume_Training = True #@param {type:\"boolean\"}\n","\n","if resume and not Resume_Training:\n","  print('\u001b[1;31mOverwrite your previously trained model ? answering \"yes\" will train a new model, answering \"no\" will resume the training of the previous model?  yes or no ?\u001b[0m')\n","  while True:\n","    ansres=input('')\n","    if ansres=='no':\n","      Resume_Training = True\n","      break\n","    elif ansres=='yes':\n","      Resume_Training = False\n","      resume= False\n","      break\n","\n","while not Resume_Training and MODEL_NAME==\"\":\n","  print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n","  time.sleep(5)\n","\n","#@markdown  - If you're not satisfied with the result, check this box, run again the cell and it will continue training the current model.\n","\n","MODELT_NAME=MODEL_NAME\n","\n","UNet_Training_Steps=1500 #@param{type: 'number'}\n","UNet_Learning_Rate = 2e-6 #@param [\"2e-5\",\"1e-5\",\"9e-6\",\"8e-6\",\"7e-6\",\"6e-6\",\"5e-6\", \"4e-6\", \"3e-6\", \"2e-6\"] {type:\"raw\"}\n","untlr=UNet_Learning_Rate\n","\n","#@markdown - These default settings are for a dataset of 10 pictures which is enough for training a face, start with 500 or lower, test the model, if not enough, resume training for 150 steps, keep testing until you get the desired output, `set it to 0 to train only the text_encoder`.\n","\n","Text_Encoder_Training_Steps=450 #@param{type: 'number'}\n","\n","#@markdown - 200-450 steps is enough for a small dataset, keep this number small to avoid overfitting, set to 0 to disable, `set it to 0 before resuming training if it is already trained`.\n","\n","Text_Encoder_Concept_Training_Steps=0 #@param{type: 'number'}\n","\n","#@markdown - Suitable for training a style/concept as it acts as heavy regularization, set it to 1500 steps for 200 concept images (you can go higher), set to 0 to disable, set both the settings above to 0 to fintune only the text_encoder on the concept, `set it to 0 before resuming training if it is already trained`.\n","\n","Text_Encoder_Learning_Rate = 1e-6 #@param [\"2e-6\", \"1e-6\",\"8e-7\",\"6e-7\",\"5e-7\",\"4e-7\"] {type:\"raw\"}\n","txlr=Text_Encoder_Learning_Rate\n","\n","#@markdown - Learning rate for both text_encoder and concept_text_encoder, keep it low to avoid overfitting (1e-6 is higher than 4e-7)\n","\n","trnonltxt=\"\"\n","if UNet_Training_Steps==0:\n","   trnonltxt=\"--train_only_text_encoder\"\n","\n","Seed=''\n","\n","ofstnse=\"\"\n","Offset_Noise = False #@param {type:\"boolean\"}\n","#@markdown - It should help with the overall quality, when used, increase the amount of the total steps by a 10-20%.\n","if Offset_Noise:\n","  ofstnse=\"--offset_noise\"\n","\n","External_Captions = False #@param {type:\"boolean\"}\n","#@markdown - Get the captions from a text file for each instance image.\n","extrnlcptn=\"\"\n","if External_Captions:\n","  extrnlcptn=\"--external_captions\"\n","\n","Style_Training = False \n","\n","#Further reduce overfitting, suitable when training a style or a general theme, don't check the box at the beginning, check it after training for at least 1000 steps. (Has no effect when using External Captions)\n","\n","Style=\"\"\n","if Style_Training:\n","  Style=\"--Style\"\n","\n","Resolution = \"512\" #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"]\n","Res=int(Resolution)\n","\n","#@markdown - Higher resolution = Higher quality, make sure the instance images are cropped to this selected size (or larger).\n","\n","fp16 = True\n","\n","if Seed =='' or Seed=='0':\n","  Seed=random.randint(1, 999999)\n","else:\n","  Seed=int(Seed)\n","\n","GC=\"--gradient_checkpointing\"\n","\n","if fp16:\n","  prec=\"fp16\"\n","else:\n","  prec=\"no\"\n","\n","s = getoutput('nvidia-smi')\n","if 'A100' in s:\n","  GC=\"\"\n","GCUNET=GC\n","if Res<=640:\n","  GCUNET=\"\"\n","\n","precision=prec\n","\n","resuming=\"\"\n","if Resume_Training and os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n","  MODELT_NAME=OUTPUT_DIR\n","  print('\u001b[1;32mResuming Training...\u001b[0m')\n","  resuming=\"Yes\"\n","elif Resume_Training and not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n","  print('\u001b[1;31mPrevious model not found, training a new model...\u001b[0m')\n","  MODELT_NAME=MODEL_NAME\n","  while MODEL_NAME==\"\":\n","    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n","    time.sleep(5)\n","\n","Enable_text_encoder_training= True\n","Enable_Text_Encoder_Concept_Training= True\n","\n","if Text_Encoder_Training_Steps==0 :\n","   Enable_text_encoder_training= False\n","else:\n","  stptxt=Text_Encoder_Training_Steps\n","\n","if Text_Encoder_Concept_Training_Steps==0:\n","   Enable_Text_Encoder_Concept_Training= False\n","else:\n","  stptxtc=Text_Encoder_Concept_Training_Steps\n","\n","#@markdown ---------------------------\n","Save_Checkpoint_Every_n_Steps = True #@param {type:\"boolean\"}\n","Save_Checkpoint_Every=500 #@param{type: 'number'}\n","if Save_Checkpoint_Every==None:\n","  Save_Checkpoint_Every=1\n","#@markdown - Minimum 200 steps between each save.\n","stp=0\n","Start_saving_from_the_step=500 #@param{type: 'number'}\n","if Start_saving_from_the_step==None:\n","  Start_saving_from_the_step=0\n","if (Start_saving_from_the_step < 200):\n","  Start_saving_from_the_step=Save_Checkpoint_Every\n","stpsv=Start_saving_from_the_step\n","if Save_Checkpoint_Every_n_Steps:\n","  stp=Save_Checkpoint_Every\n","#@markdown - Start saving intermediary checkpoints from this step.\n","\n","Disconnect_after_training=True #@param {type:\"boolean\"}\n","\n","#@markdown - Auto-disconnect from google colab after the training to avoid wasting compute units.\n","\n","def dump_only_textenc(trnonltxt, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps):\n","    \n","    !accelerate launch /content/diffusers/examples/dreambooth/train_dreambooth.py \\\n","    $trnonltxt \\\n","    $extrnlcptn \\\n","    $ofstnse \\\n","    --image_captions_filename \\\n","    --train_text_encoder \\\n","    --dump_only_text_encoder \\\n","    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n","    --instance_data_dir=\"$INSTANCE_DIR\" \\\n","    --output_dir=\"$OUTPUT_DIR\" \\\n","    --instance_prompt=\"$PT\" \\\n","    --seed=$Seed \\\n","    --resolution=512 \\\n","    --mixed_precision=$precision \\\n","    --train_batch_size=1 \\\n","    --gradient_accumulation_steps=1 $GC \\\n","    --use_8bit_adam \\\n","    --learning_rate=$txlr \\\n","    --lr_scheduler=\"linear\" \\\n","    --lr_warmup_steps=0 \\\n","    --max_train_steps=$Training_Steps\n","\n","def train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps):\n","    clear_output()\n","    if resuming==\"Yes\":\n","      print('\u001b[1;32mResuming Training...\u001b[0m')\n","    print('\u001b[1;33mTraining the UNet...\u001b[0m')\n","    !accelerate launch /content/diffusers/examples/dreambooth/train_dreambooth.py \\\n","    $Style \\\n","    $extrnlcptn \\\n","    $ofstnse \\\n","    --image_captions_filename \\\n","    --train_only_unet \\\n","    --save_starting_step=$stpsv \\\n","    --save_n_steps=$stp \\\n","    --Session_dir=$SESSION_DIR \\\n","    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n","    --instance_data_dir=\"$INSTANCE_DIR\" \\\n","    --output_dir=\"$OUTPUT_DIR\" \\\n","    --captions_dir=\"$CAPTIONS_DIR\" \\\n","    --instance_prompt=\"$PT\" \\\n","    --seed=$Seed \\\n","    --resolution=$Res \\\n","    --mixed_precision=$precision \\\n","    --train_batch_size=1 \\\n","    --gradient_accumulation_steps=1 $GCUNET \\\n","    --use_8bit_adam \\\n","    --learning_rate=$untlr \\\n","    --lr_scheduler=\"linear\" \\\n","    --lr_warmup_steps=0 \\\n","    --max_train_steps=$Training_Steps\n","\n","\n","if Enable_text_encoder_training :\n","  print('\u001b[1;33mTraining the text encoder...\u001b[0m')\n","  if os.path.exists(OUTPUT_DIR+'/'+'text_encoder_trained'):\n","    %rm -r $OUTPUT_DIR\"/text_encoder_trained\"\n","  dump_only_textenc(trnonltxt, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps=stptxt)\n","\n","if Enable_Text_Encoder_Concept_Training:\n","  if os.path.exists(CONCEPT_DIR):\n","    if os.listdir(CONCEPT_DIR)!=[]:\n","      clear_output()\n","      if resuming==\"Yes\":\n","        print('\u001b[1;32mResuming Training...\u001b[0m')\n","      print('\u001b[1;33mTraining the text encoder on the concept...\u001b[0m')\n","      dump_only_textenc(trnonltxt, MODELT_NAME, CONCEPT_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps=stptxtc)\n","    else:\n","      clear_output()\n","      if resuming==\"Yes\":\n","        print('\u001b[1;32mResuming Training...\u001b[0m')\n","      print('\u001b[1;31mNo concept images found, skipping concept training...')\n","      Text_Encoder_Concept_Training_Steps=0\n","      time.sleep(8)\n","  else:\n","      clear_output()\n","      if resuming==\"Yes\":\n","        print('\u001b[1;32mResuming Training...\u001b[0m')\n","      print('\u001b[1;31mNo concept images found, skipping concept training...')\n","      Text_Encoder_Concept_Training_Steps=0\n","      time.sleep(8)\n","\n","if UNet_Training_Steps!=0:\n","  train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps=UNet_Training_Steps)\n","\n","if UNet_Training_Steps==0 and Text_Encoder_Concept_Training_Steps==0 and Text_Encoder_Training_Steps==0 :\n","  print('\u001b[1;32mNothing to do')\n","else:\n","  if os.path.exists('/content/models/'+INSTANCE_NAME+'/unet/diffusion_pytorch_model.bin'):\n","    prc=\"--fp16\" if precision==\"fp16\" else \"\"\n","    !python /content/diffusers/scripts/convertosdv2.py $prc $OUTPUT_DIR $SESSION_DIR/$Session_Name\".ckpt\"\n","    clear_output()\n","    if os.path.exists(SESSION_DIR+\"/\"+INSTANCE_NAME+'.ckpt'):\n","      clear_output()\n","      print(\"\u001b[1;32mDONE, the CKPT model is in your Gdrive in the sessions folder\")\n","      if Disconnect_after_training :\n","        time.sleep(20)\n","        runtime.unassign()\n","    else:\n","      print(\"\u001b[1;31mSomething went wrong\")\n","  else:\n","    print(\"\u001b[1;31mSomething went wrong\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"iVqNi8IDzA1Z","colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"status":"error","timestamp":1678537113306,"user_tz":-420,"elapsed":344,"user":{"displayName":"Dang Viet Chung","userId":"10350166343994224850"}},"outputId":"1a039f5a-a1d9-4f0f-ad75-c48935eaa418"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-3370ba26e8df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mSessions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/MyDrive/Fast-Dreambooth/Sessions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m s = widgets.Select(\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}],"source":["#@markdown #Free Gdrive Space\n","\n","#@markdown Display the list of sessions from your gdrive and choose which ones to remove.\n","\n","import ipywidgets as widgets\n","\n","Sessions=os.listdir(\"/content/gdrive/MyDrive/Fast-Dreambooth/Sessions\")\n","\n","s = widgets.Select(\n","    options=Sessions,\n","    rows=5,\n","    description='',\n","    disabled=False\n",")\n","\n","out=widgets.Output()\n","\n","d = widgets.Button(\n","    description='Remove',\n","    disabled=False,\n","    button_style='warning',\n","    tooltip='Removet the selected session',\n","    icon='warning'\n",")\n","\n","def rem(d):\n","    with out:\n","        if s.value is not None:\n","            clear_output()\n","            print(\"\u001b[1;33mTHE SESSION \u001b[1;31m\"+s.value+\" \u001b[1;33mHAS BEEN REMOVED FROM YOUR GDRIVE\")\n","            !rm -r '/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/{s.value}'\n","            s.options=os.listdir(\"/content/gdrive/MyDrive/Fast-Dreambooth/Sessions\")       \n","        else:\n","            d.close()\n","            s.close()\n","            clear_output()\n","            print(\"\u001b[1;32mNOTHING TO REMOVE\")\n","\n","d.on_click(rem)\n","if s.value is not None:\n","    display(s,d,out)\n","else:\n","    print(\"\u001b[1;32mNOTHING TO REMOVE\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/TheLastBen/fast-stable-diffusion/blob/main/fast-DreamBooth.ipynb","timestamp":1678405528603}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0c39bc7ba65749f4920359f455747b88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4289a084ef314e67887a651fda611a0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afbec932a846468bbb523cdf931681b1","IPY_MODEL_deecd0d84df74bf4be3d04e63fc78fb6"],"layout":"IPY_MODEL_77762cc76da94bc8978a15a9fa8698e5"}},"77762cc76da94bc8978a15a9fa8698e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f1664da538b47f3b15a0f8728365f5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a10ac6bcc2c342d18a6fb2383d1581c0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afbec932a846468bbb523cdf931681b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"SelectModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SelectModel","_options_labels":["Select an instance image to caption"],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"SelectView","description":"","description_tooltip":null,"disabled":false,"index":0,"layout":"IPY_MODEL_a10ac6bcc2c342d18a6fb2383d1581c0","rows":25,"style":"IPY_MODEL_9f1664da538b47f3b15a0f8728365f5e"}},"deecd0d84df74bf4be3d04e63fc78fb6":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_0c39bc7ba65749f4920359f455747b88","msg_id":"","outputs":[]}}}}},"nbformat":4,"nbformat_minor":0}